根据工具返回的内容，nvidia-smi 命令主要用于查看 NVIDIA GPU 的状态和使用情况，通常在需要监控 GPU 资源或排查与 GPU 相关的问题时使用。该命令可以显示 GPU 的型号、驱动版本、CUDA 版本、GPU 温度、功耗、显存使用情况以及当前占用 GPU 的进程信息。例如，在提供的文件内容中，可以看到 nvidia-smi 显示了 GPU 0 正在使用 98% 的计算资源，占用 1542MiB 显存，并且有一个 Python 进程在使用 1539MiB 显存。此外，还提到通过 nvidia-smi 可以查看 GPU 的利用率，但某些软件如 LAMMPS、GROMACS 等可能无法查询，这可能与作业调度系统有关。对于跨节点运行的 VASP 计算，需要在 SBATCH 脚本中指定每个节点使用的 GPU 数量和 CPU 数量，并通过 nvidia-smi 查看 GPU 的使用情况。如果节点处于忙碌状态，可能会出现“Requested nodes are busy”的提示。综上，nvidia-smi 是一个非常实用的工具，可以帮助用户了解 GPU 的运行状态和资源分配情况。1. nvidia-smi 命令的基本用法是直接在终端输入 `nvidia-smi`，即可查看当前系统中所有 NVIDIA GPU 的详细信息，包括型号、驱动版本、CUDA 版本、温度、功耗、显存使用情况等。2. 使用 nvidia-smi 后可以获得的信息包括：GPU 的型号、驱动版本、CUDA 版本、GPU 温度、功耗、显存使用情况（已用/总显存）、GPU 利用率（计算资源使用百分比）以及当前占用 GPU 的进程 ID 和进程名称。例如，在文件内容中，可以看到 GPU 0 的显存使用为 1542MiB，GPU 利用率为 98%，并且有一个 Python 进程在使用 1539MiB 显存。3. 在特定的集群环境中，如 k80 集群，可以通过 `yhrun jobid=<job_id> nvidia-smi` 命令查询特定作业的 GPU 利用情况。例如，用户可以通过此命令查看 VASP 作业是否成功分配并使用了 GPU 资源，而其他软件如 LAMMPS、GROMACS 等可能无法查询，这可能与作业调度系统的限制有关。如果节点处于忙碌状态，可能会出现“Requested nodes are busy”的提示，此时需要等待节点空闲后再进行查询。4. 在跨节点运行的应用程序（如 VASP）中，需要在 SBATCH 脚本中指定每个节点使用的 GPU 数量和 CPU 数量，例如设置 `#SBATCH gpus-per-node=2` 和 `#SBATCH cpus-per-gpu=1`，然后通过 `mpirun -oversubscribe -np 2 $EXE` 启动 MPI 并行任务。提交作业后，可以通过 SSH 登录到计算节点并运行 `nvidia-smi` 查看 GPU 的使用情况。5. 如果发现 GPU 资源未被充分利用，例如用户程序仅使用了 GPU 的 25% 计算资源，建议进行计算调整以提高效率。例如，可以优化代码逻辑、增加并行化程度或调整参数配置，以更好地利用 GPU 资源。同时，需要注意 GPU 的功耗和温度，避免因过热或过载导致性能下降或硬件损坏。

