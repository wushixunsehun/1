{
    "query": "xattr_cache 参数的作用是什么？当前在客户端上通过xattr接口访问LSoM数据时，如何禁用该参数？",
    "summaries": [
        "Lustre 文件系统中，脚本通毅使用通配符统一管理客户端参数。文件 readahead 和目录 statahead 用于预读数据和元数据，提升访问效率。readahead 在顺序读取时触发，控制最大预读量的参数包括 `max_read_ahead_mb` 和 `max_read_ahead_per_file_mb`。目录 statahead 提高目录遍历性能，相关参数有 `statahead_max` 和 `statahead_agl`。OSS 读缓存通过 Linux 页面缓存提高性能，适用于多客户端读取场景，可通过 `read_cache_enable` 控制是否启用。",
        "OSS 通过读缓存和写通缓存机制优化数据访问。读缓存（read_cache）在处理相同数据的读取请求时，直接使用内存中的数据，提升性能；当禁用时，数据在读取后会被丢弃。写通缓存（writethrough_cache）控制写入数据是否保留在内存中供后续读取，适用于需要立即访问刚写入数据的场景。readcache_max_filesize 参数限制缓存文件的最大大小，适用于小文件重复访问的工作负载。异步日志提交（sync_journal）可提高性能，但可能丢失未提交的数据，需结合恢复功能使用。",
        "Lustre 2.11 引入了 MDT 的 Lazy 大小 (LSoM) 功能，用于在 MDS 上存储文件大小信息，以减少客户端访问多个 OST 获取文件大小的开销。LSoM 数据可能不准确，但能提升性能。用户可通过 `lfs getsom` 命令查看 LSoM 数据，并通过 `lfs som_sync` 同步数据。LSoM 适用于策略引擎等场景，可加快文件大小获取速度。此外，Lustre 2.11 还引入了文件级冗余 (FLR)，允许将文件数据存储在多个 OST 上，提高系统容错性和读取性能。FLR 通过延迟写入实现，主镜像更新后，其他镜像需手动同步。"
    ],
    "contents": [
        "要禁用 readahead, tf设置max_ read ahead mb=0。* llite.fsname instance.max read ahead per file mb一当获取到文件上的读取顺序时，用于控制客户端应该预读取的最大数据兆字布数 (MiB).是每文件的预读取限制，不能大于max_readq ahead mb。* llite.fsname-instance.max read ahead whole mb 一用于控制完整读取文件的最大大小〈无论read () 的大小) 。这避免了在读取整个文件之前无法有效获取顺序读取模式时对相对较小的文件的多个 RPC 读取。默认值为2 MiB 或一个RPC 的大小 如max_pPages_pet_rpc 中给定的值)。39.4.2.2. 目录 Statahead FJ AGL 的调试”许多系统命令 (Mls -LI、dqu和findq) 按顺序遍历目录。为使这些命令高效运行，可以启用目录 statahead 来提高目录遍历性能。statahead 相关可调参数有:* statahead max 一用于控制由 statahead 线程预取的最大文件属性数量。statahead默认局用，statahead max默认为 32 个文件。禁用 statahead，请在客户端上设置 =statahead max0 :lctl set Param llite.*.statahead_max=0在客户端上更改最大 statahead 窗口大小:lctl Set Param llite.*.statahead_max=n最大statahead max 为8192 个文件。目录 statahead 线程同时也会从 OST 预取文件大小或块属性，以便应用程序需要时获取客户端上的所有文件属性。这是由异步 glimpse 锁 (AGL) 设置控制，可通过以下命令禁用 AGL 行为lctl set Param llite.*.statahead_agl=0* statahead stats 一只读接口，可提供当前 statahead 和 AGL 统计信息，如目上次挂载以来已触发 statahead/AGL 的次数、由于预测错误或其他原因导致的statahead/AGL 故障次数等。注意AGL 处理的inode 是由 statahead 线程构建的，AGEL 行为因此受 statahead 的影响。如果禁用了 statahead，则 AGL",
        "对相同数据的读取请求时，OSS 将跳过从磁盘读取数据的步又，直接使用绥存中的数据完成请求。读取绥存由 Linux 内核在该 0SS 上的所有 OST上进行全局管理，以便可用内存量不足时从内存中删除最近最少使用的绥存页面。ORAS [read cache (read cache enable=0)，则 OSS 在完成客户端读取请求后丢径数据。处理后续读取请求时，OSS 将再次从磁盘读取数据。在 OSS 的所有 OST 上禁用readq_cache ，请运行:495\nLustre 文件系统操作手册 译者: 李硕root@ossl# lctl set param obdfilter.*.read_ cache enable=0重新在 OST 上局用readq_cache ，请运行:root@ossl# lctl set param obdfilter. {OST name}.read_ cache enable=1# A ltt OSS 的所有 OST 上都司用了read_cache，请运行:root@ossl# lctl get param obdfilter.*.read_ cache enable。 writethrough cache enable 一用于控制发送到 OSS 的写入请求数据是保留在读缓存用于后续读取，还是在写入完成后从绥存中丢弃。默认情况下为司用状AS (writethrough cache enable=1).当 OSS 从客户端接收写请求时，它从客户器接收数据至其内存中并将数据写入磁王。如果司用了writethrough_cache ，则此数据在写入请求完成后将保留在内存中。如果收到相同数据的后续读取请求或部分页面写和请求，OSS 可跳过从磁盘读取此数据的步桑。如果禁用了writethrougnh cache (writethrough cache enabled=0), JlOSS 在完成客户端的写入请求后丢弃数据。处理后续读取请求或部分页面写入请求时，OSS 必须从磁一重新读取数据。当客户端正在执行小数据写入或会导致部分页面更新的未对齐写入，或者其他蔬氮需要立即访问另一个节氮刚写入的文件时，建议司用writethrough_cache。例如，在生产者 -消费者 VO 模型、不同节点的 IO 操作未在 4096 字节边界上对齐的共享文件写入等",
        "或其他原因导致的statahead/AGL 故障次数等。注意AGL 处理的inode 是由 statahead 线程构建的，AGEL 行为因此受 statahead 的影响。如果禁用了 statahead，则 AGL 也会被禁494\nLustre 文件系统操作手册 译者:这ay39.4.3. OSS 读缓存的调试OSS 读绥存功能在 OSS 上提供数据的只读缓存，通过 Linux 页面缓存来存储数据。它会使用分配的所有物理内存。OSS 读绥存可在以下情况提高 Lustre 文件系统性能:。许多客户端访问相同的数据集 (如在 HPC 应用程序中或无盘客户端从 Lustre 文件系统引导时)。”一个客户站正在存储数据，而另一个客户端正在读取数据《〈即客户端通过 OST 交换数据)。© 客户端目身的缓存非常有限。OSS 读缓存提供了以下好处:\"允许 OST 更频标地绥存读取数据。。 改进重复读取以匹配网络速度而不是磁盘速度。\"提供构建 OST 写缓存〈小数据写入聚合) 的块。39.4.3.1. OSS 读缓存的使用 0SS 读缓存是在 OSS 上实现的，不需要客户端的任何特殊支持。由于 OSS 读缓存使用 Linux 页面缓存中可用的内存，因此应根据 IO 模式来确定适当的缓存内存量。如果主要是读取数据，则需要比主要为写入的 IO 模式需要更多LAE.可使用以下可调参数管理 OSS 读绥存:。 read_cache enable 一用于控制在读取请求期间从磁盘读取的数据是售保留在内存，以便于应付随后对相同数据的读取请求而无需从磁盘重新读取。默认情况下为局用状态 (read_cache_ enable=1).当 OSS 从客户端收到读取请求时，它会将数据从磁盘读取到其内存中，并将数据作为对该请求的回复。如果局用了read_cache，则在满足客户端请求后，此数据将保留在内存中。当接收到后续对相同数据的读取请求时，OSS 将跳过从磁盘读取数据的步又，直接使用绥存中的数据完成请求。读取绥存由 Linux 内核在该 0SS 上的所有 OST上进行全局管理",
        "需要立即访问另一个节氮刚写入的文件时，建议司用writethrough_cache。例如，在生产者 -消费者 VO 模型、不同节点的 IO 操作未在 4096 字节边界上对齐的共享文件写入等例子中，司用writethrough_cache可能会非常有用。相反，当大部分 IO 为文件写入且在短时间内不会被重新读取，或者文件仅由同一节点写入和重新读取时，无论 VO 是否对齐，建议禁用writethrough_cache。要在 OSS 的所有 OST 上禁用writethrough_ cache，请运行:root@ossl# lctl set param obdfilter.*.writethrough cache enable=0重新在 OST 上局用writethrough_ cache，请运行:root@ossl# lctl set param obdfilter.{OST name}.writethrough cache enable=1查看此 OSS 的所有 OST La Fa fwritethrough cache，请运行:root@ossl# lctl get param obdfilter.*.writethrough cache enable* readcache max filesize一用于控制eadq_cache和writethrough cache试保留在内存中的文件的最大大小。大于r*eadcache max filesize的文件，无论进行读取或写入，都不会保存在缓存中。设置此可调参数对于多个客户端重复访问相对较小的文件的工作负载〈如作业局动文件，可执行文件，日志文件等) 非常有用。由于大型文件只能读取或写入一次，如果不将较大的文件放入缓存中，则更多较小的文件能在缓存中保留更长的时间。490\nLustre 文件系统操作手册 译者:设置readcache _ max filesize时，输入值可以以字刷为单位指定，也可以使用后缀来指示其他二进制单位〈如玉《〈干字节)、M OB). G (PIES). T (大字TH). P (FIBF TH) )。在 OSS 的所有 OST 上将最大绥存文件大小限制为 32 MB ，请运行:root@ossl# lctl set param obdfilter.*.readcache max filesize=32MteaX{£ OST 上禁用readcache max filesize，请运行:root@ossl# lctl set param obdfilter",
        "仍可以使用默认的 DoM 布局在现有目录中创建。(Lustre 2.11 中引入)第二十一章 MDT 的 Lazy 大小功能 (LSoM)21.1. 简介在 Lustre 文件系统中，MDS 上存储着 ctitme、mtime、所有者和其他文件属性。OSS上则存储着每个文件使用的块的大小和数量。要获得正确的文件大小，客户端必须访问存储文件的每个 OST，这意味着当一个文件在多个 OST 上分条时，需要使用多个 RPC来获取文件的大小和块。MDT 上的 Lazy 大小 (LSoM) 功能将文件的大小存储在 MDS上，如果应用程序能接受获取的文件大小不精准，则可以避免访问多个 OST 以获取文件大小。Lazy 意味着不能保证存储在 MDS 上的属性的准确性。由于许多 Lustre 安装环境都使用固态硬盘作为 MDT，因此 LSoM 的目标是通过将数据存储在 MDT 上来加快从 Lustre 文件系统获取文件大小所需的时间。我们和希望Lustre 策略引擎初始使用这一功能，以扫描后端 MDT 存储，或根据不同的大小做出诀策，且不依赖于完全准确的文件大小。类似的例子还包括 Lester, Robinhood, Zester 和供应商提供的许多工具。未来将改进为允许通过1fs finq等工具访问 LSoM 数据。21.2. 启动 LSoM当使用策略引擎扫搞 MDT fa SEN, LSoM 始终处于局用状态，不需要做任何操作来启用获取 LSoM 数据的功能。通过1fs getsom命令也可以访问客户端上的LSoM 数据。因为当前在客户端上通过 xattr 接口访问 LSoM 数据，所以只要缓存了索引251\nLustre 文件系统操作手册 译者: 李硕Tid, xattr_cache 就会在客户端上绥存文件大小和块计数。在大多数情况下，这是可行的，因为它改善了对 LSoM 数据的访问频率。但是，这也意味着，如果在首次访问 xattr后文件大小发生了变化，或者在首次创建文件后不久访问 xattr，LSoM 数据可能会过时。如果需要访问过时的最近 LSoM 数据，可以在客户端通过1ct1 set_param1dlm.namespaces.xmqdqcx.1LIru size=clear取消MDC 锁定，刷新",
        "创建文件后不久访问 xattr，LSoM 数据可能会过时。如果需要访问过时的最近 LSoM 数据，可以在客户端通过1ct1 set_param1dlm.namespaces.xmqdqcx.1LIru size=clear取消MDC 锁定，刷新 xattr 2. A则，如果在 LDLM 锁定超时前未访问文件，则将从客户端缓存中删除文件属性。通过LIct1l get param 1ldlm.namespaces.*mdc*.lru_max_ age储存锁定超时时长如果从特定客户端 (如 HSM 代理节点) 重复访问最近创建或频繁修改的文件的LSoM 属性，则可以使用lctl set param llite.*.xattr_ cache=0来禁用客户wi LAY xattr 缓存。但这可能会导致在访问文件时的额外开销，一般不建议使用。21.3. 用户命令Lustre 提供了1fs getsom命令以显示存储在 MDT 上的文件属性。11som_sync命令人允许用户将MDT 上的文件属性与 OSTs 上的有效或最新数据同步。可以在具有 Lustre 文件系统载入点的客户端上调用11som_sync命令。该命令使用Lustre MDS 变更日志，因此必须注册变更日志用户才能使用此命令工具。21.3.1 使用Lfs getsom显示 LSoM 数据lis getsom命令列出了存储在 MDT 上的文件属性。调用该命令需使用 Lustre 文件系统上文件的完整路径和文件名。如果没有使用选项，则存储在 MDS 上的所有文件属性都将显示出来。21.3.2 lfs getsom 命令1 1fs getsom [-s] [-b] [-f] <filename下面列出了各种 岂 getsom 选项。选项 说明-s ，仅显示给定文件的LSoM 数据的大小值。这是一个可选标志-pb ， 仅显示给定文件的LSoM 数据的块值。这是一个可选标志-£ ， 仅显示给定文件的 LSoM 数据的标志值。这是一个可选标志。有效的标志值有: SOM_FL_ UNKNOWN = 0x0000 ，表示未知或没有 SoM 数据，必须从 OSTS 获取大小; SOM _FL STRICT = 0x0001，表示已知且严格正确",
        "标志值有: SOM_FL_ UNKNOWN = 0x0000 ，表示未知或没有 SoM 数据，必须从 OSTS 获取大小; SOM _FL STRICT = 0x0001，表示已知且严格正确，252\nLustre 文件系统操作手册这aX选项”说明FLR 文件 (SOM 保证) ; SOM_FL_DEISE = 0x0002，表示已知但已过时，即在过去的某个时间点是正确的，但现在已知 (或可能) 不正确 (例如，打开进行写入); SOM_FL_LAZY = 0x0004，表示近似值，可能从未严格正确过，需要同步 SOM 数据以实现最终的一致性。第二十二章文件级元余 (ELR)22.1. 概述Lustre 文件系统最初就是为 HPC 而设计的，筷一直在具备内部元余性和容销性的高端存储上运行归好。然而，尽管这些存储系统的成本昂贵、结构复杀，存储必障仍然时有发生。事实上，在 Lustre 2.11 RA ZH, Lustre 文件系统并不比其底层的单个存储AUR ae LE EAT SE. Lustre 文件系统并没有机制能够缓解硬件存储改隐。当服务融无法访问或终止服务时，将无法访问文件。Lustre 2.11 中引入了 Lustre 文件级元余 (FLR) 功能，任何 Lustre 文件都可将相同的数据存储在多台 OST 上，以提升系统在存储故障或其它故障发生时的稳健性。在存在多个针像的情况下，可选择最合适的镜像来啊应单个请求，这对 IO 可用性有直接影啊。此外，对于许多客户闯同时读取的文件〈如输入版，共孚库或可执行文件)，可以通过创建文件数据的多个镜像来提高单个文件的并行聚合读取性能。第一阶段的FLR 功能通过延迟写入实现〈如\"图 21.1 FLR EIR GA\" 所示)。在写入镜像文件时，只有一个主镜像或首选镜像在写入过程中直接更新，而其他镜像将被标记为stale。通过使用命令行工具《由用户或管理员直接运行或通过目动监控工具运行)同步各镜像之间同步，该文件可在随后再次写入其它镜像。Object j (primary, preferred)delayed resync图 25: FLR delay writting图",
        "root@ossl# lctl set param obdfilter.*.readcache max filesize=32MteaX{£ OST 上禁用readcache max filesize，请运行:root@ossl# lctl set param obdfilter. {OST name}.readcache max filesize=-1l查看是否 OSS 的所有0OST Laila FA freadcache max filesize，请运行:root@ossl# lctl get param obdfilter.*.readcache max filesize39.4.4. 启用 OSS 异步日志提交OSS 异步日志提交功能将数据异步地写入磁盘，而不强制进行日志刷新。这将减少搜索次数，并显著提高了某些硬件的性能。注意异步日志提交不能用于直接的 IO 发起的写入〈设置了oO_DIRECT标志)。在这种情况下，将强制执行日志刷新。局用异步日志提交功能后，客户端节点会将数据保留在页面绥存中《页面引用)。Lustre 客户端将监视从 OSS 发送到客户端的消息中的最后提交的交易号 (transno)。当客户端看到 OSS 报告的最后一个提交的tr*ansno至少等于批量写入的trzansno时，它会在相应的页面上释放引用。为避免批量写入后客户端上的页面引用时间过长，在收到批量写入的回复后将发起 7 秒的 ping XK (OSS 文件系统提交默认时间间隔为 3 BD),以便 OSS 报告最后提交的transno。如果 OSS 在日志提交之前崩溃，则中间数据将丢失。但是，结合异步日志提交的OSS 恢复功能能够使客户端重放其写入请求，并通过恢复文件系统的状态来补偿丢失的磁盘更新。默认情况下，sync_journal为启用状态 (sync_journal=1)，以便同步提交日记条目。局用异步日志提交，请输入以下内容将sync_journal参数设置为 0:—$ lctl set_param obdfilter.*.sync_journal=02 obdfilter.lol-OST0001.sync_journal=0AKA sync-on-lock-cancel 功能〈黑认司用) WRIT 2 he Pi Be BS入对象的交叉区域后的 OSS 及其中一个客户端朋省时可能导致的数据不一致问题。当违反连续写入的",
        "脚本通毅会使用通配符“或文件系统专用的通配符 fname-* 来统一指定所有客户端上的参数设置。比如说1 lctl get_param osc.testfs-OST0000-osc-fffF88107412f400.rpc_ stats2 osc.testfs-OST0000-osc-ffff88107412F400.rpc_stats=3 snapshot time: 1375743284 .337839 (secs.usecs)4 read RPCs in flight: 05 write RPCs in flight: 039.4.2. 文件 Readahead 和目录 Statahead 的调试文件 readahead 和目录 statahead 人允许在进程请求数据之前将数据读入内存。文件readahead 将文件内容预取到内存中以进行与ead () 相关调用，而目录 statahead 将文件元数据提取到内存中以进行readqir ()和stat ()相关调用。当 readahead 和 statahead运行良好时，访问数据的进程可在请求时立即在客户端的内存中获取所需的信息，而没有网络 IO HEIR.39.4.2.1. 文件 Readahead = *4 Linux 绥神区高速缓存中的数据无法满足应用程序的两个或更多顺序读取时，将触发文件 readahead。初始预读的大小由 RPC 大小和文件条市大小决定，通首至少为1 MB ，附加的预读将线性增长并递增，直到客户端上的预读绥存到达了每个文件或每个系统的预置量绥存限制。Readahead 相关可调参数有:* llite.fsname-instance.max read ahead mb 一用于控制文件预读的最大数据量。在文件描述符上第二次顺序读取之后，预读文件至 RPC 大小的块(4MiB 或更大的read () 大小) 中。随机读取的大小只能为reaq () 调用大小《无TSE) 。读取文件至非连续区域会重置预读算法，并且在再次顺序读取之前不会再次触发预读。493\nLustre 文件系统操作于册 译者:DCZR At这是对所有文件的全局限制，不能大于客户端RAM 的12。要禁用 readahead, tf设置max_ read ahead mb=0。* llite.fsname instance.max read ahead per file mb一当获取到文件上的读取顺序时，用于控制"
    ]
}