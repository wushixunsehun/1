{
    "query": " inode 数量耗尽会导致什么后果？如何解决？",
    "summaries": [
        "为使用 ldiskfs 格式的 OST 指定非默认的 inode ratio 可能导致索引节点总数超过限制，从而引发空间超限错误，浪费空间并降低 e2fsck 速度。应使用默认 inode ratio 以确保系统正常运行。OST 文件系统检查时间受多种因素影响，正常情况下每 TiB 需 5-30 分钟，若存在大量错误则时间会增加。Lustre 文件系统有多个极限值，如最大 MDTs 数量、OSTs 数量、OST 大小、客户端数量等，这些值受架构和系统限制，部分可通过重新编译修改。文件条带化、文件大小、目录文件数等也有限制，具体数值因文件系统类型（如 ldiskfs 或 ZFS）而异。Lustre 支持大文件和大量文件，但实际容量受限于 OST 空间和配置。",
        "gerris在单节点运行多作业时效率低下，三个作业共享前4个核心导致资源争用。无论是否为独占节点，问题均存在。通过绑核操作可解决此问题。用户使用yhrun启动作业，但未进行核心绑定，导致性能下降。需使用taskset等工具将作业绑定到特定核心，以提高运行效率。",
        "系统计算节点使用三种内核版本（ft2k、ft3k、mt3k），各内核及其驱动源码目录不同。计算节点镜像生成包括清空目录、拷贝基础文件及配置文件，最终生成initrd镜像并上传至PXE服务器。镜像缺少libevent-dev库，需通过定位文件、确认所属包、查询包内容等方式解决缺失问题。"
    ],
    "contents": [
        "查看\n三个作业共享了前4个核计算。抢占式节点和独占节点都存在同样的问题。需进行绑核操作！\ntop - 20:06:23 up 106 days, 19:09,  1 user,  load average: 12.00, 12.00, 12.00\nTasks:  24 total,  13 running,  11 sleeping,   0 stopped,   0 zombie\n%Cpu0  :100.0 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu1  :100.0 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu2  :100.0 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu3  :100.0 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu4  :  0.3 us,  0.0 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu5  :  0.3 us,  0.0 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu6  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa",
        "include/evdns.h\n/usr/include/event.h\n/usr/include/event2\n/usr/include/event2/buffer.h\n/usr/include/event2/buffer_compat.h\n/usr/include/event2/bufferevent.h\n/usr/include/event2/bufferevent_compat.h\n/usr/include/event2/bufferevent_ssl.h\n/usr/include/event2/bufferevent_struct.h\n/usr/include/event2/dns.h\n/usr/include/event2/dns_compat.h\n/usr/include/event2/dns_struct.h\n/usr/include/event2/event-config.h\n/usr/include/event2/event.h\n/usr/include/event2/event_compat.h\n/usr/include/event2/event_struct.h\n/usr/include/event2/http.h\n/usr/include/event2/http_compat.h\n/usr/include/event2/http_struct.h\n/usr/include/event2/keyvalq_struct.h\n/usr/include/event2/listener.h\n/usr/include/event2/rpc.h\n/usr/include/event2/rpc_compat.h\n/usr/include/event2/rpc_struct.h\n/usr/include/event2/tag.h\n/usr/include/event2/tag_compat.h\n/usr/include/event2/thread.h\n/usr/include/event2/util.h\n/usr/include/event2/visibility.h\n/usr/include/evhttp.h\n/usr/include/evrpc.h\n/usr/include/evutil.h\n/usr/lib\n/usr/lib/aarch64-linux-gnu\n/usr/lib/aarch64-linux-gnu/libevent.a\n/usr/lib/aarch64-linux-gnu/libevent_core.a\n/usr/lib/aarch64-linux-gnu/libevent_extra.a\n/usr/lib/aarch64-linux-gnu/libevent_openssl.a\n/usr/lib/aarch64-linux-gnu/libevent_pthreads.a\n/usr/lib/aarch64-linux-gnu/pkgconfig\n/usr/lib/aarch64-linux-gnu/pkgconfig/libevent.pc\n/usr/lib/aarch64",
        "上的单个文件大小最大为 16 TiB。在 64 位系统上，这个限制不存在。因此，如采后备文件系统可以文持足够大的对象或者文件很稀蕊，则文件大小可以是2 * 63位 〈8EiB)。单个文件最多可以有 2000 个条市，这使得 64 位 ldiskfs 系统的单个文件能达到 31.25 PiB。的容量文件中可存储的实际数据量取决于文件条市化所在的 OST 中的可用空间量。Lustre 软件使用 ldiskfs 哈希目录代码，依赖于文件名长度，一个目录下最多能包含大约一千万个文件。子目录与闻规文件相同。(在 Lustre 2.8中引入) ，注意从 Lustre2.8 开始，可通过1fs mkdir -c命令将多个 MDTS 上的单个目录条带化来突破此限制，使用多少目录条市数则该最大文件或子目录数量就可以增加多少倍。Lustre55\nLustre 文件系统操作手册详这aX名称 值文件系统上 40 亿/MDT最大文件数 (ldiskfs)，量 256 万亿/MDT(ZFS)最长文件名 255 bytes最长路径名 4096 bytesLustre 文 无限制件系统上当前打开的文件最大数量注意描述文件系统已测试了单个目录下 1000 万个文件。Idiskfs 文件系统的上限为 40 亿个 inodes。默认情况下，MDT 文件系统为每个 node 格式化 2KB空间，即每1TiB MDT 空间有 5.12 亿个 inode。这可以在MDT 文件系统创建时进行初始化。ZFS OVE RANT ACA S| Rk, FE MDT 空间LATER SITAR. ES RG RARE大约 4KiB 的镜像空间，具体取决于配置。每个附加的 MDT 都可容纳上述最大数量的附加文件，这取雇于文件系统中的可用空间以及分布目录和文件。包括底层文件系统在内，单个文件名的最大限制W255 Fo受 Linux VFS 限制，最长路径名为 4096 字HeWoLustre 软件对打开的文件数量疫有限制，但实际上，它还是受制于于 MDS 上的内存大小。MDS 上没有所谓当前打开文件的\" SUR\",为它们只与给定客户端的接口相链接。每个客户端进程最多能打开几王个文件，这取决于它的ulimit。默认情况下，ldiskfs",
        "【已解决】gerris单节点多作业运行效率差，绑核解决\n**标签**: gerris 单节点多作业 taskset 绑核\n**创建时间**: 2022-07-21 11:27:41\n**更新时间**: 2022-07-21 11:27:41\n**作者**: 刘栋杰\ngerris单节点多作业运行效率差\n加载环境\n#!/bin/bash\nmodule purge\nmodule add Intel_compiler/16.0.3\nmodule add MPI/mvapich2-2.2/intel2016u3\nmodule add proj/4.9.3-icc16\nmodule add gsl/2.3-icc16\nmodule add netcdf/4.4-icc16-mvapich2\nmodule add hypre/2.11.1-icc16-mvapich2\nmodule add ode/0.13-sp-icc16\nmodule add fftw/3.3.4-icc16-mvapich2\nmodule add gts/121130-icc16\nmodule add gnuplot/5.2.5\nmodule add libffi/3.2.1-icc16\nmodule add glib/2.40.2-icc16\nexport GERRIS_HOME=/THL7/home/liudj/lib/gerris/icc-16-mvapich2\nexport PATH=$GERRIS_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$GERRIS_HOME/lib:$LD_LIBRARY_PATH\nexport PKG_CONFIG_PATH=$GERRIS_HOME/lib/pkgconfig:$PKG_CONFIG_PATH\n运行脚本\n#!/bin/bash\nEXE='gerris2D split.gfs'\nDIRS='/THL7/home/liudj/test/gerris'\ncore=4\nfor job in {1..3}\ndo\ncd ${DIRS}/${job}\ntime yhrun -n ${core}   exclusive   ${EXE} &\n# -D 指定使用计算位置也可以\ndone\nwait\n计算节点运行效率  top  按1 查看\n三个作业共享了前4个核计算。抢占式节点和独占节点都存在同样的问题。需进行绑核操作！\ntop - 20:06:23 up 106 days, 19",
        "5.1.12 镜像更新\n5.1.12.1 镜像说明\n当前系统计算节点使用3种内核版本，分别为ft2k、ft3k、mt3k，其中各自内核源码以及相对应驱动源码目录如下\nft2k主目录/home/sys/ft2k/\nft2k内核源码linux-5.4.0-65-ft2k/\nft2k flash驱动源码ft2kp_flash/\nft2k lustre源码lustre-2.14.0/\nft2k zni驱动源码zni-glex-3.26/\nft2k xpmem源码xpmem/\nft3k主目录/home/sys/ft3k/\nft3k内核源码linux-5.4.0-65-ft3k/\nft3k lustre源码lustre-2.14.0/\nft3k zni驱动源码zni-glex-3.26/\nft3k xpmem源码xpmem/\nmt3k主目录/home/sys/mt3k_651_new/\nmt3k内核源码linux-5.4.0-65-mt-cpm3/\nmt3k flash驱动源码mt3k_flash/\nmt3k lustre源码lustre-2.14.0/\nmt3k zni驱动源码zni-glex-3.26/\nmt3k dsp驱动源码mod/\n计算节点镜像\n计算节点镜像主目录/home/sys/cn_651_new/\n以下目录均为编译生成配置文件\n镜像基础文件initram/\n镜像总成目录initram_tmp/\n内核模块kernel/\ndsp驱动dsp-mt/\nflash驱动flash/\nglusterfs转发程序glusterfs/\nlustre程序客户端lustre-2.14.0-cn/\nslurm程序openpmix-3.2.2/\nslurm-19.05.7-cn-with-pmix-3.2.2/\nslurm-20.11.7-cn-with-pmix-3.2.3/\n用户管理程序lam-yhpc\nnss-yhpc\nUcx+mpi程序ucx-mpich-ompi\nzni驱动zni-glex-3.26-cn/\nYhclushd程序yhclushd\n系统配置文件总成sysconf/\n镜像目录生成：先清空initram_tmp/目录，首先完整拷贝initram/*至initram_tmp/*，再依次根据genram内顺序，将\nkernel \\\nflash \\\ndsp-mt \\\nlustre-2.14.0-cn \\\nlustre-force-rmmod \\\nzni-glex-3.26-cn \\\nknem \\\nopenpmix-3.2.3 \\\nslurm-19.05.7-cn-with-pmix-3.2.3 \\",
        "--mkfsoptions=\"-i $((8192 *1024))\" …注意使用 ldiskfs 格式化的 OST 不能超过最多 3.2 (LPR. 401 ESI. AKAOST 指定一个非彰小的 inode ratio，因而导致索引节点总数超出最大值，将导致过早地出现空间超限错误，OST 空间不能被完全使用，浪费空间，使 e2fsck 速度变慢。因此，请选择默认的 inode ratio，以确保索引和点的总数仍然低于这个限制。OST 文件系统检查时间受到包括索引和点数量在内等一系列变量的影响，如文件系统的大小、分配的块数量、分配块在磁盘上的分布、磁玛速度、CPU GREE. AR ae EA内存数量。对于正靖运行的文件系统，合理的文件系统检查时间大概在每 TiB 5-30 分钟左右，但如果检测到大量错误并需要修正，时间则会显若增加。53\nLustre 文件系统操作手册译者:这ay5.4. 文件和文件系统的极限值下表描述了当前已知 Lustre 相关了最大指标值。这些值受限于 Lustre 体系结构、Linux虚拟文件系统 (VFS) 或虚拟内存子系统。其中少数值是在代码中定义的，通过重新编译Lustre 软件可以进行更改。可利用以下例子中这些极限值测试 Lustre 软件。名称最大 MDTs数量最大 OSTs数量最大 OST大小最大客户器数量最大单个文件系统大小最大条人带数值2308150512TiB(Idiskfs),512TiB (ZFS)131072至少 1EiB2000描述一个MDS 可以承载多个MDT，每个MDT 可以是一个单独的文件系统。最多可以将 255 个MDTs 添加到文件系统，并使用 DNE 远程或条带目录将其附加到名称空间中。OST 的最大数量是一个可以在编译时改变的浓量。Lustre 文件系统已经测试了多达 4000 个 OSTs.ZB OST 文件系统可以配置在单个 OSS Fi AE.这不是一个硬性限制。也可以配置更大的 OST，但是大多数生产系统通常不会超过该限制，为 Lustre 可以通过增加视外的 OSTs 来提升容量和人性能以及I/0 总体性能，尽量减少竞争并多许并行恢复 〈e2fsck Bk scrub) .对于 32 位内核，由于页面缓存限制，",
        "0.0 hi,  0.0 si,  0.0 st\n%Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu8  :  0.3 us,  0.3 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu9  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu10 :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu11 :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu12 :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu13 :  0.3 us,  0.3 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu14 :  0.3 us,  0.3 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu15 :  1.3 us,  0.7 sy,  0.0 ni, 98.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\n%Cpu16 :  0.3 us,  0.3 sy,  0.0 ni,",
        "\\\nlustre-force-rmmod \\\nzni-glex-3.26-cn \\\nknem \\\nopenpmix-3.2.3 \\\nslurm-19.05.7-cn-with-pmix-3.2.3 \\\nslurm-19.05.7-plus \\\nucx-mpich-ompi \\\nlam-yhpc \\\nnss-yhpc \\\nyhrms-yhpc \\\nyhclushd \\\nglusterfs \\\nsysconf目录下文件\n依次拷贝至initram_tmp/*内，再针对initram_tmp/*文件进行压缩打包，生成initrd镜像文件，并将该文件转移至pxe server覆盖同名文件\n5.1.12.2 镜像系统缺少相关lib库\n举例，计算节点系统缺少libevent-dev相关类库支持\n确认缺少哪个文件\n[root@cn160%xx ~]#\nlibevent.so: No such File or Directory\n在登录节点，确认该类库所在绝对路径\n[root@ln27%xx ~]# locate libevent.so\n/usr/lib/aarch64-linux-gnu/libevent.so\n或者\n[root@ln27%xx ~]# find /usr -name libevent.so\n/usr/lib/aarch64-linux-gnu/libevent.so\n反向确认该类库文件属于哪个deb包\n[root@ln27%xx ~]# dpkg-query -S /usr/lib/aarch64-linux-gnu/libevent.so\nlibevent-dev: /usr/lib/aarch64-linux-gnu/libevent.so\n或\n[root@ln27%xx ~]# dpkg-query -S /lib/aarch64-linux-gnu/libevent.so\nlibevent-dev: /usr/lib/aarch64-linux-gnu/libevent.so\n某些deb包安装时，lib库路劲使用/lib，而非/usr/lib路径\n再查询该deb包安装时共提供哪些文件\n[root@ln27%xx ~]# dpkg-query -L libevent-dev\n/usr\n/usr/include\n/usr/include/evdns.h\n/usr/include/event.h\n/usr/include/event2\n/usr/include/event2/buffer.h\n/usr/include/event2/buffer_compat.h\n/usr/include",
        "可以通过增加视外的 OSTs 来提升容量和人性能以及I/0 总体性能，尽量减少竞争并多许并行恢复 〈e2fsck Bk scrub) .对于 32 位内核，由于页面缓存限制，最大块设备大小为 16TB ，这个大小也适用于 OST。强烈建议使用 64 位内核运行 Lustre 客户端和服务需。客户端的最大数量是一个可以在编译时改变的种量。在生产环境中使用了高达 30000 个客户端。每个 OST 可将其文件系统配置成最大 OST 大小，并且可将所允许的最大数量的 OSTs 组合成单个文件系统。该值受存储在磁盘上并以RPC 请求形式发送的布局信息大小限制，但这不是协议中的硬性限制。文件系统中的 OST 数量可以超过条带数量，单个54\nLustre 文件系统操作手册这ay名称 值最大条市大 <4GiB小By/)SitrK 64 KiB小最大单个对“16TiB象大小 (Idiskfs),256TiB (ZFS)最大文件大 16TiB (32小 位系统) 31.25PiB(64 位Idiskfs 系统)，8EiB (64 位ZFS 系统)单个目录下 1000 万个文件最大文件或 (Idiskfs), 2°48子目录效量 个文件 (ZFS)描述文件条带化的 OST 数量将受限于此。在移动到下一个对象前写入到每个对象的数据量。由于在某些 64 位机器 (如 ARM 和POWER) 上的 64 KiBPAGE SIZE 限制，最小条市大小被设置为 64KiB。这样单个页面就不会被拆分到多个服务硕上即可以存储在单个对象中的数据量。一个对象对应一个条带。ldiskfs 的限制为 16 TB, we AA TA个对象。对于 ZFS，该限制来目于底层 OST 的大小。文件最多可以包含 2000 个条带，每个条带可达到的最大对象大小。SARA EF KBR, FE 32 位系统上的单个文件大小最大为 16 TiB。在 64 位系统上，这个限制不存在。因此，如采后备文件系统可以文持足够大的对象或者文件很稀蕊，则文件大小可以是2 * 63位"
    ]
}