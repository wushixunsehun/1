{
    "query": "某用户作业因“forrtl: severe (41): insufficient virtual memory”报错中断，可能的原因及解决方法是什么？",
    "summaries": [
        "TH1A用户运行Fortran程序时出现“Segmentation fault - invalid memory reference”错误，经排查为内存溢出导致。解决方案是在编译时添加-g选项，并使用valgrind工具检查内存泄漏。编译命令为：gfortran Matrix.f90 -L/vol6/software/libraries/lapack/3.8.0-gcc49/lib64 -llapack -lblas -g，随后运行valgrind进行内存检查。",
        "系统日志显示多次出现“GLEX create region failed: no enough memory resources”错误，表明内存资源不足。随后发生MPI通信错误，导致任务被终止。最终因内存不足，程序在执行能量最小化时崩溃，提示“Not enough memory. Failed to realloc...”。命令行使用了768个MPI进程和64个OpenMP线程，可能因资源分配不合理导致内存不足。解决思路为MPI传输数据量过大，需优化资源分配或减少并发数。",
        "本文分析了计算节点多进程程序在内存充足情况下出现“cannot allocate memory”错误的原因。主要原因是Linux系统对内存的过量分配机制（overcommit），在使用`os.fork()`创建子进程时，虽然物理内存未满，但虚拟地址空间可能被耗尽，导致OOM错误。解决方案包括调整`/proc/sys/vm/overcommit_memory`参数或改用多线程程序。"
    ],
    "contents": [
        "glex_md.c:362  UCX  ERROR GLEX create region failed: no enough memory resources\n[1639011636.916846] [cn1024:2865294:0]         glex_md.c:362  UCX  ERROR GLEX create region failed: no enough memory resources\n[1639011636.917635] [cn1024:2865294:0]         glex_md.c:362  UCX  ERROR GLEX create region failed: no enough memory resources\n[1639011636.918398] [cn1024:2865294:0]         glex_md.c:362  UCX  ERROR GLEX create region failed: no enough memory resources\n[1639011636.919190] [cn1024:2865294:0]         glex_md.c:362  UCX  ERROR GLEX create region failed: no enough memory resources\n[1639011636.919993] [cn1024:2865294:0]         glex_md.c:362  UCX  ERROR GLEX create region failed: no enough memory resources\n[1639011636.920777] [cn1024:2865294:0]         glex_md.c:362  UCX  ERROR GLEX create region failed: no enough memory resources\n[1639011636.921564] [cn1024:2865294:0]         glex_md.c:362  UCX  ERROR GLEX create region failed: no enough memory resources\nAbort(671210510) on node 613 (rank 613 in comm 0): Fatal error in PMPI_Sendrecv: Message truncated, error stack:\nPMPI_Sendrecv(243): MPI_Sendrecv(sbuf=0x8f56390, scount=12, MPI_BYTE, dest=427, stag=0, rbuf=0x8f563a8, rcount=12, MPI_BYTE, src=43, rtag=0, comm",
        "per rank.\nProgram:     gmx mdrun, version 2018.8\nSource file: src/gromacs/utility/smalloc.cpp (line 226)\nMPI rank:    444 (out of 768)\nFatal error:\nNot enough memory. Failed to realloc 2058442216 bytes for\nnbs->work[thread].sort_work, nbs->work[thread].sort_work=0\n(called from file\n/thfs1/home/kanbw/gromacs-version/package/gromacs-2018.8-float/src/gromacs/mdlib/nbnxn_grid.cpp,\nline 1322)\nFor more information and tips for troubleshooting, please check the GROMACS\nwebsite at http://www.gromacs.org/Documentation/Errors\nAbort(1) on node 444 (rank 444 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 444\nslurmstepd: error: *** STEP 324037.0 ON cn1024 CANCELLED AT 2021-12-13T17:02:29 ***\nyhrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nyhrun: error: cn3944: task 633: Killed\nyhrun: error: cn2612: task 444: Aborted\nEnergy minimization. End.\nCommand line:\ngmx_mpi mdrun -v -deffnm 1aki_em -npme 256 -ntomp 64 -dd 8 8 8\nBack Off! I just backed up 1aki_em.log to ./#1aki_em.log.2#\nReading file 1aki_em.tpr, VERSION 2018.8 (single precision)\nNOTE: disabling dynamic load balancing as it is only supported with dynamics, not with integrator 'cg'.\nUsing 768 MPI processes\nUsing 64 OpenMP threads per MPI",
        "=0x8f56390, scount=12, MPI_BYTE, dest=427, stag=0, rbuf=0x8f563a8, rcount=12, MPI_BYTE, src=43, rtag=0, comm=0x84000001, status=0xfffffa9d8ad8) failed\n(unknown)(): Message truncated\n[cn4052:2872045:0:2872045] Caught signal 11 (Segmentation fault: address not mapped to object at address (nil))\nslurmstepd: error: *** STEP 321183.0 ON cn1024 CANCELLED AT 2021-12-09T09:00:37 ***\nyhrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\nyhrun: error: cn3711: task 272: Killed\nEnergy minimization. End.\n解决思路\n目前显示应该是MPI传输数据量太大，导致中断。尚未还没有较好的思路。\nCommand line:\ngmx_mpi mdrun -v -deffnm 1aki_em -npme 256 -ntomp 64 -dd 8 8 8\nBack Off! I just backed up 1aki_em.log to ./#1aki_em.log.3#\nReading file 1aki_em.tpr, VERSION 2018.8 (single precision)\nNOTE: disabling dynamic load balancing as it is only supported with dynamics, not with integrator 'cg'.\nUsing 768 MPI processes\nUsing 64 OpenMP threads per MPI process\nNOTE: Your choice of number of MPI ranks and amount of resources results in using 64 OpenMP threads per rank, which is most likely inefficient. The optimum is usually between 1 and 6 threads per rank.\nProgram:     gmx mdrun, version 2018.8\nSource file: src/gromacs/utility/smalloc.cpp (line 226)\nMPI rank:",
        "上下文环境，也会尝试创建自己的`40GB`虚拟内存地址空间。因此，理论上在创建两个子进程之后，就会导致虚拟内存地址空间耗尽，进而导致进程创建失败，但在实际返回时，错误显示`Cannot allocate memory`信息。\n相关的内存地址空间分配信息可以通过`grep -i commit /proc/meminfo`查看，例如如下信息：\nCommitLimit:    73955212 kB\nCommitted_AS:   1230403 kB\n其中，`CommitLimit`代表当前系统**可以申请的总内存**，而`Committed_AS`代表当前**已经申请**的内存。\n在监测报错程序的内存开销时，就会发现，在报错时，`Commited_AS`的开销在超过`CommitLimit`的限制时，机会出现`Cannot allocate memory`错误。\n解决方案\n通过原因分析，我们可以发现，这个问题的出现主要是看系统对于内存空间申请和物理内存空间占用的管理策略问题。Linux默认是允许`memory overcommit`的，只要你来申请内存我就给你，寄希望于进程实际上用不到那么多内存，但万一用到那么多了呢？Linux设计了一个OOM killer机制挑选一个进程出来杀死，以腾出部分内存，如果还不够就继续。\n1. 解决方案1\n由系统管理员调整系统对于`overcommit`的处理策略，具体设置在`/proc/sys/vm/overcommit_memory`文件中，默认策略为`0`，可选的策略包括如下三种（[linux 内存分配限制,overcommit_memory 2](https://blog.csdn.net/qq_16097611/article/details/52816908)）：\n+ 0 — 默认设置。内核执行启发式内存过量使用处理，方法是估算可用内存量，并拒绝明显无效的请求。遗憾的是因为内存是使用启发式而非准确算法计算进行部署，这个设置有时可能会造成系统中的可用内存超载；\n+ 1 — 内核执行无内存过量使用处理。使用这个设置会增大内存超载的可能性，但也可以增强大量使用内存任务的性能；\n+ 2 — 内存拒绝等于或者大于总可用swap大小以及  overcommit_ratio指定的物理RAM比例的内存请求。如果您希望减小内存过度使用的",
        "【已解决】TH1A用户运行Fortan程序报错：Segmentation fault - invalid memory reference\n**标签**: 无标签\n**创建时间**: 2021-10-13 14:26:03\n**更新时间**: 2021-12-09 11:24:30\n**作者**: 杜思慧\n**运行编译后的a.out报错：**\nProgram received signal SIGSEGV: Segmentation fault - invalid memory reference.\nBacktrace for this error:\n#0  0x2ab6b24e5222\n#1  0x2ab6b24e596e\n#2  0x39c9a3291f\n#3  0x400ecf\n#4  0x400e24\n#5  0x400e5a\n#6  0x39c9a1ecdc\n#7  0x400b98\nyhrun: error: cn4922: task 0: Segmentation fault\n经查该错误是由于内存溢出引起的\n**解决方案：**\n在编译时加上-g，再利用valgrind检查内存泄漏\n编译指令：\ngfortran Matrix.f90 -L/vol6/software/libraries/lapack/3.8.0-gcc49/lib64 -llapack -lblas -g\n编译后得到a.out，运行：```\nvalgrind tool=memcheck leak-check=yes ./a.out",
        "【已解决】计算节点多进程程序cannot allocate memory问题原因分析\n**标签**: fork, 多进程, oom, out of memory\n**创建时间**: 2022-05-19 18:35:10\n**更新时间**: 2022-05-19 18:37:30\n**作者**: 傅浩\n**问题**：计算节点采用多进程运行程序时，出现free显示有足够内存，但是提示OOM问题，导致程序终止。\n问题描述\n之前在使用python处理数据时，处理代码用到了python的`multiprocessing`包里的进程池技术，但在底层调用`os.fork()`接口创建新的进程时，会出现`cannot allocate memory`错误信息，但是**实际上物理内存并没有用满**，导致程序执行失败。\n原因分析\n1. 系统内存分配机制\n在Linux系统中，对于物理内存的实际分配发生在读写操作时，需要触发系统的**缺页故障**，才能实际分配内存，在实际调用`malloc`类似操作时，在未对内存进行操作时，实际上并没有分配物理内存，而只是分配了一个虚拟地址空间。\n在得知系统对于内存分配的机制之后，就可以解释为什么调用`free`工具查看内存消耗时，显示有大量物理内存空闲，或者在调用`ulimit -a`时，发现`max memroy size`为不受限。\n2. 进程创建机制\n在调用系统`os.fork()`接口创建新的进程时，由于理论上进程具有独立性，因此，无法与创建其的父进程共享同一内存地址空间，需要创建相同与父进程相同的上下文执行环境，即也需要创建相同大小的虚拟内存地址空间，但是实际上并没有分配物理内存空间。例如：假设父进程需要消耗`40GB`内存空间，系统物理内存+swap空间共`120GB`，即地址空间大小为`120GB`，在执行`os.fork()`时，子进程会拷贝父进程的上下文环境，也会尝试创建自己的`40GB`虚拟内存地址空间。因此，理论上在创建两个子进程之后，就会导致虚拟内存地址空间耗尽，进而导致进程创建失败，但在实际返回时，错误显示`",
        "但也可以增强大量使用内存任务的性能；\n+ 2 — 内存拒绝等于或者大于总可用swap大小以及  overcommit_ratio指定的物理RAM比例的内存请求。如果您希望减小内存过度使用的风险，这个设置就是最好的。\n可以通过系统设置的方法，将`overcommit_memory`的设置修改为1，即可解决由虚拟内存地址空间需求超过实际物理内存空间导致的`Cannot allocate memory`错误信息，但是需要程序员自己了解自己的程序实际不会用到超过物理内存空间大小的内存，否则可能导致其他错误。\n2. 解决方案2\n将多进程程序修改为多线程程序，即可避免由于非必要内存空间分配导致的`OOM`错误。\n参考资料\n1. [内存不足：OOM](https://www.zhangzhuo.ltd/articles/2021/08/10/1628565705959.html)\n2. [linux - fork() failing with Out of memory error](https://ogeek.cn/qa/?qa=990926/)\n3. [linux 内存分配限制,overcommit_memory 2](https://blog.csdn.net/qq_16097611/article/details/52816908)\n4. [Linux内存充足会出现oom,LINUX内存OOM哪些事](https://blog.csdn.net/weixin_39869197/article/details/116697433)"
    ]
}