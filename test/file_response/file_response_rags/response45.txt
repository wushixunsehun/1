{
    "query": "更换存储硬盘时，如何通过命令确认硬盘恢复状态？",
    "summaries": [
        "执行硬盘更换操作，包括标记、下线、清除设备名、更换硬盘及恢复。过程中需注意报警信息，确认硬盘状态。更换后通过zpool status查看恢复状态，待所有硬盘Online后关闭硬盘灯。操作成功标志为无错误信息，硬盘恢复正常运行。",
        "本文主要介绍了在ZFS存储池中更换硬盘的步骤和注意事项。首先需确认坏盘是否存在别名，如slot-13，并检查存储池状态，若显示为数字则需使用GID换盘。换盘命令格式为`zpool replace -f <池名> <旧盘> <新盘>`，若新盘别名更改，可直接使用新别名。若换盘失败，提示存在ZFS文件系统，需用`labelclear`清除配置。换盘后会进行数据同步，可通过`zpool status`查看进度。对于有热备盘的存储池，可使用热备盘替换坏盘。",
        "文本主要描述了存储系统中磁盘状态信息及更换坏盘的操作流程，强调可直接更换新盘而不需调整热备盘位置。同时介绍了如何查看ZFS数据集的后台数据，包括卸载、设置canmount属性及重新挂载等步骤，以便运维人员进行数据管理和调整。系统环境为Red Hat 7.6，使用Lustre 2.12.0和ZFS 0.7.13。"
    ],
    "contents": [
        "issued at 690M/s, 27.eT total”,\n\nSTATE READ WRITE CKSUM\",\n\n\"\\tosti9-5DEGRADED = @ Ss\",\n\n\"At raidz2-@DEGRADED = @ Ss\",\nJBOD19-S46 ONLINEee 8 en，\nBoD19-s47 。 ONLINEee 8 eB,\nJBOD19-S48 。 ONLINEee 8 eB,\n3B0D19-S49 。 ONLINEee 8 eB,\n3BOD19-S5@_ ONLINEee 8 en，\n]Bop19-ss1 。 ONLINEee 8 eB,\nBoD19-ss2 。 ONLINEee 8 eB,\n380D19-S53_ ONLINEee a\",\nreplacing-8 DEGRADED @ 9 6\noldOFFLINE 101e\n3B0D19-S54_ ONLINEe@_(resilvering)”,\nJBOD19-S55___ ONLINE9 9 6\n\nrrors: No known data errors”\n\nPLAY. RECAP S00; aso Oo ESOS EEE BO BEBO ERE IOOCBEOO UE GO RESO CEE IOC ESOC GEO IOE\n\n89.72.103.18: ok=2changed=1 。 unreachable=-8failed=@ 。 skipped-8 。 rescued-8 —ignored-0\n图中指JBOD19-S54还有4分钟恢复完毕。\n9）盘恢复完毕后关闭硬盘灯\n报警消失，查看zpool status状态，盘都是ONLINE状态。\noss18坦询zpool油状态 X\n\nwith @ errors on Mon Mar 11 11:35:08 2024\",\n\nSTATE | READ WRITE CKSUM\"”\n“\\tost19-5ONLINE日\nAt raidz2-8ONLINE\n\"At 3B0D19-546| ONLINE\n\"At 3B0D19-547| ONLINE\n3B0D19-s48| ONLINE\n3B0D19-S49] ONLINE\n3B0D19-s56| ONLINE\n3B0D19-S51] ONLINE\n3B0D19-S52] ONLINE\n3B0D19-S53] ONLINE\n3B0D19-S54] ONLINE\n3B0D19-S55] ONLINE\n\neeecesceecee000\nooooooooeooa\noaeoaoaeoeaeoeaeaoae\n\n“errors: No known data errors”\n\nPLAY. RECAP S00; aso Oo",
        "2changed=1 。 unreachable=-8failed=@ 。 skipped-8 。 rescued-8 —ignored-0\n\n口 “执行结果:成功\n通过查询得到硬盘符为sdbm。\n4）标记硬盘\n根据JBOD19-S54点亮硬盘。\n行标记硬盘操作吗?\n\n硬盘 JBOD19-S54\n\n动作 | RE\n© PLAY [al1] xzrrrrrrrsrrrrrrrrrsrrrrrrrrrrrrrrrrerrrrrrrrerrrerrrrrerrrrrrrrrerr\n\n© changed: [89.72.103.18]\n\n© ok: [89.72.103.18] => {\n“msg”: [\n\n” HGST H4162-] 3010\",\n\n“Enclosure Status diagnostic page:\",\n\n” INVOP=0, INFO=1, NON-CRIT=, CRIT=0, UNRECOV:\ngeneration code: exe\",\nstatus descriptor list”,\n\nElement 54 descriptor:\",\n\n.Predicted failure=0, Disabled=0, Swa\nOk=0, Reserved device=@, Hot spare=, Cons check=\nIn crit array=, In failed array-0, Rebuild/remap=0, R/R abort:\nApp client bypass A=®, Do_not_remove=®, Enc bypass A=®, Enc bypass B-8\",\nheady to insert, tv-e[ Toeneei Reporte”,\nApp client bypass 8-0, Fault sensed-0, Fault regstd-0, Device of!\n.Bypassed A=®, Bypassed B=@, Dev bypassed A=0, Dev bypassed B-0\"\n\nPLAY. RECAP S00; aso Oo ESOS EEE BO BEBO ERE IOOCBEOO UE GO RESO CEE IOC ESOC GEO IOE\n\n89.72.103.18: ok=2changed=1 。 unreachable=-8failed=@ 。 skipped-8 。 rescued-8 —ignored-0\n操作执行成功即可，Ident=1表示硬盘处于点亮状态。\n5）下线硬盘\nJBOD19-S54",
        "版本\n- 系统：redhat7.6\n- 文件系统：Lustre：2.12.0\n- ZFS：0.7.13\n5.5.2、目的\n为运维人员查看后台数据，做相应调整，提供方便。\n5.5.3、方法\n- 前提（以 2 个存储池 mds 和 ost 为例）：\n存在 2 个存储池 mds 和 ost，“Mount type”是“zfs”，且被格式化为 lustre 文件系统的数据集为 mds/mds，ost/ost（可以是其他），mds 是元数据存储池。 mds/mds 和 ost/ost 均以 lustre 形式挂载。mds 和 ost 以 zfs 形式挂载。\n# df -Th\nFilesystem     Type      Size  Used Avail Use% Mounted on\n/dev/sda1      ext4       11G  4.4G  5.8G  44% /\ndevtmpfs       devtmpfs  898M     0  898M   0% /dev\n/dev/sda2      ext4      6.8G  1.6G  4.9G  25% /home\nmds            zfs        20G     0   20G   0% /mds\nmds/mds        lustre     20G  1.9M   20G   1% /mnt/mds\nost            zfs        58G     0   58G   0% /ost\nost/ost        lustre     58G  1.8M   58G   1% /mnt/ost\n以查看设备 ost/ost 中信息为例，方法如下：\n- 卸载 ost/ost\n同一个文件系统不能以 lustre 和 zfs 同时挂载。\n# umount <挂载点或数据集>\n示例",
        "设备 ost/ost 中信息为例，方法如下：\n- 卸载 ost/ost\n同一个文件系统不能以 lustre 和 zfs 同时挂载。\n# umount <挂载点或数据集>\n示例:\n卸载以 lustre 类型挂载的存储池 ost\n# umount ost\n- 获取存储池 ost 的 canmount 属性\n#  get canmount <存储池>\n示例：\n# zfs get canmount ost\nNAME  PROPERTY  VALUE     SOURCE\nost     canmount  on        default\n存储池 ost 默认 canmount 属性为“on”状态。若为“off”状态，要设置为“on”状态。\n- 设置存储池 ost 的 canmount 属性为 on\n# zfs set canmount=on <存储池>\n示例：\n# zfs set canmount=on ost\n- 设置数据集 ost/ost 的 canmount 属性\ncanmount 属性决定了是否可以挂载、查看后台数据。\n查看文件系统 canmount 属性\n# zfs get canmount <数据集>\n示例：\n# zfs get canmount ost/ost\nNAME     PROPERTY  VALUE     SOURCE\nost/ost     canmount  off        local\nost/ost 默认 canmount 状态为“off”。要将其设置为“on”状态。\n# zfs set canmount=on <数据集>\n示例：\n# zfs get canmount ost/ost\nNAME     PROPERTY  VALUE     SOURCE\nost/ost     canmount  on        local\n此时，设备 canmount 属性为“on”状态。\n- 以 zfs 格式挂载数据集\n挂载命令：\n# zfs mount <数据集>\n示例：\n# zfs mount ost/ost\n# df -Th\nFilesystem     Type      Size  Used Avail",
        "。 unreachable=-8failed=@ 。 skipped-8 。 rescued-8 —ignored-0\n操作执行成功即可，Ident=1表示硬盘处于点亮状态。\n5）下线硬盘\nJBOD19-S54\n输入卷和硬盘来下线硬盘。脚本反馈执行成功即可。\n6）清除硬盘设备名\n将步骤5中的硬盘设备名填入对话框中，来清除盘符。\n清除硬盘设备名操作吗?\n\n设备名 | sdbm|\n7）更换硬盘\n硬盘的备件在备机（JBOD149，I/O66机柜）里面。\n在存储机柜将硬盘更换好，填入卷和硬盘，更换硬盘。\n您确定要执行更换硬瘟操作f\n\nB ostt9s\n\nwa | se0v19-ss4\n执行成功即换盘成功。如未返回成功，可能是未识别盘符，通过运维平台查看日志，判断是否有新硬盘插入，如果没有，可以再换一块盘试试。\n换盘过程中，会有如下报警，均为正常\n故障点\n\noss18\n\noss18\n\noss18\n\n故障原因\n\nthfs1-0ST0070卷降级\n\nost19-5JBOD19-S54磁盘状态异常\n\nost19-5状态异党\n\n故障级别\n\n。 严重\n\n。 严重\n\n。 严重\n将故障硬盘贴签，注明日期、JBODxx-Sxx、硬盘编号。\n8）可以通过zpool status查看恢复状态\n通过zpool status查看盘是否在恢复，以及恢复所需时间。根据存储卷使用量以及作业情况，每块盘恢复时间不等。\n要执行查询zpool池杖\n\nse [onto\noss18查鹿zpool池状态 x\n\nstate: DEGRADED\",\n\ntatus: One or more devices is currently being resilvered. The pool will\",\ntcontinue to function, possibly in a degraded state.\",\n\n‘action: Wait for the resilver to complete.”,\n\nt27.0T scanned at 695M/s, 26.8T issued at 690M/s, 27.eT total”,\n\nSTATE READ WRITE CKSUM\",\n\n\"\\tosti9-5DEGRADED = @ Ss\",\n\n\"At raidz2-@",
        "** 下是否存在新盘的别名，即 slot-13 这个链接是否存在，存在则别名生效了。\n- 换盘\n检查存储池状态，查看坏盘 slot-13 在存储池中是否显示的还是 slot-13，如果为一个数字字符串，则为存储池中该成员盘的 id（这里的 slot-13 在重启服务器后，重新导入存储池可以看到 gid 为 2823177480828651994，此时换盘就需要使用 gid 来进行换盘。\n换盘命令格式\n# zpool replace -f <存储池名字> <坏盘别名或者gid> <新盘别名>\n示例：\n按照别名换盘\n# zpool replace -f ost1 slot-13 slot-13\n按照 gid 换盘\n# zpool replace -f ost1  2823177480828651994 slot-13\n如果新盘的别名更改了，那么可以使用新盘别名进行换盘\n# zpool replace -f ost1 slot-13 <新盘别名>\n换盘时如果换盘失败，提醒说新盘中存在 zfs 文件系统，此时可以使用以下方法清除配置信息：\n清除配置信息命令：\n# zpool labelclear -f <硬盘的绝对路径>\n示例:\n这里假设硬盘别名为 slot-13\n如果采用多路径，那么硬盘的绝对路径即为： /dev/mapper/硬盘名\n# zpool labelclear -f /dev/mapper/slot-13\n如果使用的是虚拟设备 vdev，那么硬盘的绝对路径为： /dev/disk/by-vdev/硬盘名-part1\n# zpool labelclear -f /dev/disk/by-vdev/slot-13-part1\n- 数据同步\n换盘成功后，将执行数据同步。\n# zpool status ost1\npool: ost1\nstate: DEGRADED\nstatus: One or more devices is currently being resilvered.  The pool will\ncontinue to function, possibly in a degraded state.\naction: Wait for the resilver to complete.\nscan: resilver in progress since Thu May 28 16:19:43 2020\n1.48G",
        "ONLINE       0     0     0\nslot-10  ONLINE       0     0     0\nslot-4   ONLINE       0     0     0\nslot-5   ONLINE       0     0     0\nslot-6   ONLINE       0     0     0\nslot-7   ONLINE       0     0     0\nslot-8   ONLINE       0     0     0\nslot-9   ONLINE       0     0     0\nspares\nslot-11    AVAIL\nslot-3     AVAIL\nerrors: No known data errors\n- ### 直接更换坏盘\n特别注意： 以上使用热备替换坏盘的操作流程是 解绑、换盘、添加新热备盘，所有操作完成后热备盘位置发生了变化。其实完全可以直接更换新盘，不用更改热备盘位置。操作方法：\n1、拔掉坏盘后直接在坏盘的槽位插入新盘\n2、生成新的别名（映射），参考本节上文 **更换新盘** 和 **重新生成映射** 进行操作\n3、执行换盘操作\n# zpool replace ost0 slot-3 slot-3 -f\n换盘完毕后顶替上去的热备盘 **slot-10** 会自动分离，重新成为热备盘，即状态由 **INUSE** 恢复为 **AVAIL**。此后存储池即进行数据恢复。\n5.5 查看 zfs 数据集中存储的后台数据\n5.5.1、软件版本\n- 系统：redhat7.6\n- 文件系统：Lustre：2.12.0\n- ZFS：0.7.13\n5.5.2、目的\n为运维人员查看后台数据，做相应调整，提供方便。\n5.5.3、",
        "ONLINE       0     0     0\nslot-15                ONLINE       0     0     0\nslot-16                ONLINE       0     0     0\nslot-17                ONLINE       0     0     0\nslot-18                ONLINE       0     0     0\nslot-19                ONLINE       0     0     0\nerrors: No known data errors\n5.4.2、有热备盘的存储池换盘\n注：ost0 为存储池名字 slot-x 为硬盘别名\n此处有两种换盘方法：\n- ### 使用热备替换坏盘\n1、换盘\n此时的存储池状态为：\n# zpool status ost0\npool: ost0\nstate: DEGRADED\nstatus: One or more devices could not be used because the label is missing or\ninvalid.  Sufficient replicas exist for the pool to continue\nfunctioning in a degraded state.\naction: Replace the device using 'zpool replace'.\nsee: http://zfsonlinux.org/msg/ZFS-8000-4J\nscan: resilvered 148K in 0h0m with 0 errors on Wed May 27 11:12:23 2020\nconfig:\nNAME           STATE     READ WRITE CKSUM\nost0           DEGRADED",
        "a degraded state.\naction: Wait for the resilver to complete.\nscan: resilver in progress since Thu May 28 16:19:43 2020\n1.48G scanned out of 198G at 303M/s, 0h11m to go\n140M resilvered, 0.75% done\nconfig:\nNAME                       STATE     READ WRITE CKSUM\npool6                      DEGRADED     0     0     0\nraidz2-0                 DEGRADED     0     0     0\nslot-10                ONLINE       0     0     0\nslot-11                ONLINE       0     0     0\nslot-12                ONLINE       0     0     0\nreplacing-3            DEGRADED     0     0     0\n1625424560663067014  UNAVAIL      0     0     0  was /dev/disk/by-vdev/slot-13/old\nslot-13              ONLINE       0     0     0  (resilvering)\nslot-14                ONLINE       0     0     0\nslot-15"
    ]
}