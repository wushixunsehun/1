{
    "query": "在 Lustre 文件系统中，如何通过lock_reclaim_threshold_mb和lock_limit_mb参数配置限制客户端 LDLM 锁的内存占用？",
    "summaries": [
        "Lustre 文件系统操作手册摘要：END_OFESET 选项不能与选项1同时使用，文件范围长度为 LENGTH，且不能与 -e 同时指定。Lockahead 请求模式包括 READ 和 WRITE，用于请求锁。ladvise 用于控制 LDLM 锁定行为，影响服务器端缓存管理。示例展示了如何使用 lfs ladvise 设置读取、不需或锁定提示。34.9.1 节介绍了批量 IO（16MB RPC）的优化，通过调整 brw_size 和 max_pages_per_rpc 参数提升性能。34.10 节提到提升小文件 IO 性能的方法，如 IO 聚合、使用 MPI-IO、避免锁定等。",
        "Lustre 文件系统内存需求包括客户端、MDS 和 OSS。客户端推荐至少 2GB RAM。MDS 内存需求取决于客户端数量、目录大小和负载，每个文件约占用 2KB 内存。默认日志大小为 4096MB，故障切换时需翻倍。计算示例显示，1024 个客户端、12 个交互式客户端和 600 万文件需至少 16GB RAM。OSS 内存需求包括服务线程、读取缓存等，推荐最小 32GB RAM，用于 8 个 OST 设备。额外内存可提升性能。",
        "Lustre 2.11 引入了 MDT 的 Lazy 大小 (LSoM) 功能，用于在 MDS 上存储文件大小信息，以减少客户端访问多个 OST 获取文件大小的开销。LSoM 数据可能不准确，但能提升性能。用户可通过 `lfs getsom` 命令查看 LSoM 数据，并通过 `lfs som_sync` 同步数据。LSoM 适用于策略引擎等场景，可加快文件大小获取速度。此外，Lustre 2.11 还引入了文件级冗余 (FLR)，允许将文件数据存储在多个 OST 上，提高系统容错性和读取性能。FLR 通过延迟写入实现，主镜像更新后，其他镜像需手动同步。"
    ],
    "contents": [
        "1fs ladvise -a dontneed -s 0 -e 1048576000 /mnt/lustre/filel—请求文件/mnt/Luster/filel的前1MiB AY LDLM iB, DOSER MER TPA该文件此区域的OST 请求一个锁:clientl$S lfs ladvise -a lockahead -m READ -s 0 -e 1M /mnt/lustre/filel—请求文件/mnt/Luster/filel[3 MiB, 10 MiB] 范围的LDLM 写入锁，这将尝试从保存有该文件此区域的 OST 请求一个锁:clientl$S 1fs ladvise -a lockahead -m WRITE -s 3M -e 10M /mnt/lustre/filel—34.9. 大批量 /O (16MB RPC)34.9.1. 概述从 Lustre 2.9 jf, Lustre 文持的 RPC 大小最大已扩展到 16MB。在客户端和服务器之间传输相同数量的数据，启用更大的 RPC 意味着需要更少的RPC，OSS 可以同时向底层磁盘提交更多数据，因此可以生成更大的磁盘 IO 以充分利用磁盘日益增加的带宽。在各户问连接时，客户端将与服务硕协商允许使用的最大RPC。客户端始终可以发送小于此最大值的RPC。417\nLustre 文件系统操作手册 译者: 李硕客户端可通过在OST 上使用参数brw_size来获知最大 (首选) VO 大人小。所有与此目标交互的客户端都不能发送大于此值的RPC。客户问可以通过osc.*.max_pages_per_rpc 可调参数单独设置较小的RPC 大小限制。注意可为ZFS OST 设置的最小brw_size大小即该数据集的 recordsize 大小。这可以确保客户端可以随时写入完整的 ZFS 文件块，而不会强制为每个 RPC 执行读/修改/写操作。34.9.2. 示例为了启用更大的 RPC 大小，必须将brw_size的 IO 大小值更改为 16MB。临时更改bzw_size，请在 OSS 上运行以下命令:1 oss# lctl set param obdfilter.fsname-OST* .brw_size=16",
        "分配 RPC-sized MB JIO 的缓冲区，因此不需要通过 IO 请求来分配和释放缓冲区。。0SS 读取缓存: OSS 读取缓存提供 OSS 数据的只读缓存，使用浓规的 Linux 页面缓存来存储数据。与 Linux 操作系统中的常规文件系统的缓存一样，0SS 读取绥存使用所有可用的物理内存。适用于 MDS 的计算也同样适用于从 OSS 访问的文件，但因为其负载分布在更多HY OSSs “RE, (AlKKZE MDS 下列出的锁、inode 缓存等所所需的内存数也分散在这些OSS 节点上。由于这些内存需求，应将下面的计算作为确定 OSS 节点所需的最小RAM 大小。5.5.3.1 计算 OSS 内存需求4 8 “+ OST fy OSS 的推荐最小RAM 大小计算如下: Linux 内核与用户空间和守护进程的内存 = 1024 MB 以太网/TCP 23K / REWER DX (16 MB * 512 线程)= 8192 MB 1024MB 日志大小*8个OST 设备=8192MB 每个OST IO 线程的 16 MB 读/写操作缓存* 512个线程 = 8192 MB 2048 MB 文件系统读取缓存* 8 OST = 16384 MB 1024 * 4 核客户端*1024 个文件/核* 2kB/文件 = 8192MB 12 个交互式客户端* 100,000 个文件* 2kB/文件 =2400MB 2,000,000 文件〈附加工作集) * 2kB/文件 = 4096MB DLM 锁+ 文件系统元数据总量=31072MB 每个OSS DLM 锁+ 文件系统元数据= 31072MB/4 OSS = 7768MB {iti值) 每个OSS RAM 最小需求=32 GB 〈估值)预先分配的绥神区就消耗了大约 16 GB，文件系统和内核则至少还需要附加的 1GB。因此，对于非故障切换配置，使用8 个OST 的 OSS “HY RAM 至少应为 32 GB。在 OSS 上添加额外的",
        "上的内存大小。MDS 上没有所谓当前打开文件的\" SUR\",为它们只与给定客户端的接口相链接。每个客户端进程最多能打开几王个文件，这取决于它的ulimit。默认情况下，ldiskfs MDT 单个文件的最大条市数为 160 个 OST。在格式化MDT 时使用--mkfsoptions=\"-O ea_ inode\"可增加该值，或在格式化 MDT 后使用une2fs -O ea _ inode来启用并改变它。56\nLustre 文件系统操作手册这ay5.5. 确定内存需求5.5.1 客户端内存需求推荐使用至少2 GB RAM 的客户端。5.5.2 MDS 内存需求MDS 内存需求由以下因素决定:。 客户最大数量。 目录大小。 服务器上负载情况MDS 使用的内存数量与系统中有多少客户端，以及饭们在工作集中使用多少文件有关。它主要是由客户端一次可以容纳的锁数量决定。客户端持有的锁的数量因服务需上的负载和闪存可用性而异。交互式客户端有时可以容纳超过 10,000 个锁。在 MDS 上，每个文件大约使用2KB 的内存，包括 Lustre 分布锁管理融 (DLM) 锁和当前文件的内核数据结构。与从存储读取数据相比，将文件数据放在缓存中可以提高元数据性能 10fia ESMDS 内存需求包括:“文件系统元数据: 需要合理数量的RAM 以支持文件系统元数据。虽然文件系统元数据的数量没有硬性的限制，但如果有更多的RAM 可用，则可以减少通过磁盘了O 检索元数据的频率。“网络传输: 如果您使用的是 TCP 或其他使用系统内存来发送或接收缓训的网络传输，那么也须将这些内存需求考虑在内。“日志大小: 默认情况下，用于每个 Lustre ldiskfs 文件系统的日志大小为 4096 MB.这占用了每个文件系统的 MDS A EAI Cat) RAM.。 故障切换配置: 如果 MDS 节氮用于从另一个节点进行故障转移，那么每个日志所需的RAM 应翻倍。当主服务融发生故障时，备份服务硕才有能力处理附加的负载。5.5.2.1 计算 MDS 内存需求默认情况下，文件系统日志",
        "END_OFESET。该选项不能与1 选项同时指定。文件范围长度为 LENGTH。该选项不能与-e同时指定。Lockahead 请求模式{TREAD, WRITE} 。请求一个该模式下的锁。通前，1fs ladqvise会将建议转发给 Lustre 服务禹，但无法保证何时以及哪些服务做会对建议做出反应。根据不同建议的类型以及受影啊的服务郁端组件的实时决策情况，建议可能会触发操作也可能不会触发操作。ladvise 的典型用例是使具有外部知识的应用程序和用户能够介入服务器端缓存管理。例如，如有果大量不同的客户端正在对文件进行小的随机读取，则在随机 IO AAR410\nLustre 文件系统操作手册 译者:前以大线性读取的方式预取页到 OSS 绥存的做法效益可观。由于发送到客户端的数据还要多得多，可能无法使用 fadvise0 将数据提取到每个客户端缓存中。ladvise lockahead的不同之处在于它试图通过在使用之前明确请求LDLM 锁来控制 LDLM 锁定行为。这不会直接影响缓存行为，相反，它可以在特殊情况下用于避免正省LDLM 锁定行为导致的病态结果 hia请注意，noexpandg建议适用于特定 六 ，因此通过 Is 使用它并不起作用。它只能用特定的用于 IO 的文件描述Linux 系统调用fadvise()和1Lfs ts () 只是一个各户端机制，它不会将建议传递给文件系统，而ladvise可以癌 Lustre {kas vin送建议或提示。34.8.2. 示例下面的例子中，持有第一个 1GB 的/mnt/Luster/ file1得到提示: 即将读取文件的前 1GB 部分。 °°clientlS 1fs ladvise -a willread -s 0 -e 1048576000 /mnt/lustre/filel/—下面的例子中，持有第一个 1GB 的/mnt/Luster/ filel得到提示: 文件的前1GB 部分在近期不会被读取，所以OST 可以在内存中清除该文件的绥存。clientl$S 1fs ladvise -a dontneed -s 0 -e 1048576000 /mnt/lustre/filel—请求文件/mnt/Luster/filel的前1MiB AY LDLM iB, DOSER MER TPA",
        "一个节点进行故障转移，那么每个日志所需的RAM 应翻倍。当主服务融发生故障时，备份服务硕才有能力处理附加的负载。5.5.2.1 计算 MDS 内存需求默认情况下，文件系统日志使用4096MB。额外的 RAM 用于存储更大的工作集组存文件数据，通稼它并不处于活跃状态，但应保持热度以提升访问速度。在没有锁的情况下，每个文件保存在缓存中大约需要 1.5 KB 内存。例如，在 MDS 上的单个MDT，有 1024 个客户靖、12 个交互节氮、一个 600 万个文件的工作集〈其中 400 万个文件在客户端缓存上):57\nLustre 文件系统操作手册 译者:As大操作系统开销 = 1024 MB 文件系统日志=4096MB 1024 * 4 4% Fe PF oh * 1024 个文件/核* 2KB = 4096MB 12 个交互式客户端* 100,000 个文件* 2KB = 2400 MB 2,000,000文件〈附加工作集) * 1.5kB/文件=3096 MB因此，具有这种配置的MDT 的最小需求是至少 16 GB 的RAM。但是，额外的闪存可以显者提高性能。对于包含 100 万或更多文件的目录，更多的内存大有神益。例如，当一个客户端要随机访问 1000 万个文件中的一个时，有附加的内存来进行缓存可以大大地提高性能。5.5.3 OSS AER在为一个 OSS 下氮规划硬件时，须考虑 Lustre 文件系统中几个组件的内存使用情Die CU: 上日志、服务线程、文件系统元数据等)。愉外，也须考虑 OSS 读取缓存特性，因其在 OSS 贡点上绥存数据时将消耗内存。除上文中提到的 MDS 内存需求外，OSS 的内存要求包括:。 服务线程: OSS 节点上的服务线程为每个 ost_io 服务线程预分配 RPC-sized MB JIO 的缓冲区，因此不需要通过 IO 请求来分配和释放缓冲区。。0SS 读取缓存: OSS 读取缓存提供 OSS 数据的只读缓存，使用浓规的",
        "仍可以使用默认的 DoM 布局在现有目录中创建。(Lustre 2.11 中引入)第二十一章 MDT 的 Lazy 大小功能 (LSoM)21.1. 简介在 Lustre 文件系统中，MDS 上存储着 ctitme、mtime、所有者和其他文件属性。OSS上则存储着每个文件使用的块的大小和数量。要获得正确的文件大小，客户端必须访问存储文件的每个 OST，这意味着当一个文件在多个 OST 上分条时，需要使用多个 RPC来获取文件的大小和块。MDT 上的 Lazy 大小 (LSoM) 功能将文件的大小存储在 MDS上，如果应用程序能接受获取的文件大小不精准，则可以避免访问多个 OST 以获取文件大小。Lazy 意味着不能保证存储在 MDS 上的属性的准确性。由于许多 Lustre 安装环境都使用固态硬盘作为 MDT，因此 LSoM 的目标是通过将数据存储在 MDT 上来加快从 Lustre 文件系统获取文件大小所需的时间。我们和希望Lustre 策略引擎初始使用这一功能，以扫描后端 MDT 存储，或根据不同的大小做出诀策，且不依赖于完全准确的文件大小。类似的例子还包括 Lester, Robinhood, Zester 和供应商提供的许多工具。未来将改进为允许通过1fs finq等工具访问 LSoM 数据。21.2. 启动 LSoM当使用策略引擎扫搞 MDT fa SEN, LSoM 始终处于局用状态，不需要做任何操作来启用获取 LSoM 数据的功能。通过1fs getsom命令也可以访问客户端上的LSoM 数据。因为当前在客户端上通过 xattr 接口访问 LSoM 数据，所以只要缓存了索引251\nLustre 文件系统操作手册 译者: 李硕Tid, xattr_cache 就会在客户端上绥存文件大小和块计数。在大多数情况下，这是可行的，因为它改善了对 LSoM 数据的访问频率。但是，这也意味着，如果在首次访问 xattr后文件大小发生了变化，或者在首次创建文件后不久访问 xattr，LSoM 数据可能会过时。如果需要访问过时的最近 LSoM 数据，可以在客户端通过1ct1 set_param1dlm.namespaces.xmqdqcx.1LIru size=clear取消MDC 锁定，刷新",
        "创建文件后不久访问 xattr，LSoM 数据可能会过时。如果需要访问过时的最近 LSoM 数据，可以在客户端通过1ct1 set_param1dlm.namespaces.xmqdqcx.1LIru size=clear取消MDC 锁定，刷新 xattr 2. A则，如果在 LDLM 锁定超时前未访问文件，则将从客户端缓存中删除文件属性。通过LIct1l get param 1ldlm.namespaces.*mdc*.lru_max_ age储存锁定超时时长如果从特定客户端 (如 HSM 代理节点) 重复访问最近创建或频繁修改的文件的LSoM 属性，则可以使用lctl set param llite.*.xattr_ cache=0来禁用客户wi LAY xattr 缓存。但这可能会导致在访问文件时的额外开销，一般不建议使用。21.3. 用户命令Lustre 提供了1fs getsom命令以显示存储在 MDT 上的文件属性。11som_sync命令人允许用户将MDT 上的文件属性与 OSTs 上的有效或最新数据同步。可以在具有 Lustre 文件系统载入点的客户端上调用11som_sync命令。该命令使用Lustre MDS 变更日志，因此必须注册变更日志用户才能使用此命令工具。21.3.1 使用Lfs getsom显示 LSoM 数据lis getsom命令列出了存储在 MDT 上的文件属性。调用该命令需使用 Lustre 文件系统上文件的完整路径和文件名。如果没有使用选项，则存储在 MDS 上的所有文件属性都将显示出来。21.3.2 lfs getsom 命令1 1fs getsom [-s] [-b] [-f] <filename下面列出了各种 岂 getsom 选项。选项 说明-s ，仅显示给定文件的LSoM 数据的大小值。这是一个可选标志-pb ， 仅显示给定文件的LSoM 数据的块值。这是一个可选标志-£ ， 仅显示给定文件的 LSoM 数据的标志值。这是一个可选标志。有效的标志值有: SOM_FL_ UNKNOWN = 0x0000 ，表示未知或没有 SoM 数据，必须从 OSTS 获取大小; SOM _FL STRICT = 0x0001，表示已知且严格正确",
        "标志值有: SOM_FL_ UNKNOWN = 0x0000 ，表示未知或没有 SoM 数据，必须从 OSTS 获取大小; SOM _FL STRICT = 0x0001，表示已知且严格正确，252\nLustre 文件系统操作手册这aX选项”说明FLR 文件 (SOM 保证) ; SOM_FL_DEISE = 0x0002，表示已知但已过时，即在过去的某个时间点是正确的，但现在已知 (或可能) 不正确 (例如，打开进行写入); SOM_FL_LAZY = 0x0004，表示近似值，可能从未严格正确过，需要同步 SOM 数据以实现最终的一致性。第二十二章文件级元余 (ELR)22.1. 概述Lustre 文件系统最初就是为 HPC 而设计的，筷一直在具备内部元余性和容销性的高端存储上运行归好。然而，尽管这些存储系统的成本昂贵、结构复杀，存储必障仍然时有发生。事实上，在 Lustre 2.11 RA ZH, Lustre 文件系统并不比其底层的单个存储AUR ae LE EAT SE. Lustre 文件系统并没有机制能够缓解硬件存储改隐。当服务融无法访问或终止服务时，将无法访问文件。Lustre 2.11 中引入了 Lustre 文件级元余 (FLR) 功能，任何 Lustre 文件都可将相同的数据存储在多台 OST 上，以提升系统在存储故障或其它故障发生时的稳健性。在存在多个针像的情况下，可选择最合适的镜像来啊应单个请求，这对 IO 可用性有直接影啊。此外，对于许多客户闯同时读取的文件〈如输入版，共孚库或可执行文件)，可以通过创建文件数据的多个镜像来提高单个文件的并行聚合读取性能。第一阶段的FLR 功能通过延迟写入实现〈如\"图 21.1 FLR EIR GA\" 所示)。在写入镜像文件时，只有一个主镜像或首选镜像在写入过程中直接更新，而其他镜像将被标记为stale。通过使用命令行工具《由用户或管理员直接运行或通过目动监控工具运行)同步各镜像之间同步，该文件可在随后再次写入其它镜像。Object j (primary, preferred)delayed resync图 25: FLR delay writting图",
        "IO 大小值更改为 16MB。临时更改bzw_size，请在 OSS 上运行以下命令:1 oss# lctl set param obdfilter.fsname-OST* .brw_size=16要持久地更改brw_size，请运行:1 oss# lctl set param -P obdfilter.fsname-OST* .brw_size=16当客户端连接到 OST 目标时，它将从目标中获取bzrw_size，并从brw_size中获得其最大值和本地设置作为max_pPages_per_rpc的实际了RPC 大小。因此，要启用16MB 的RPC，客户端的max pages per rpc必须设置为 16M (如果 PAGESIZE 为4KB，则为 4096) 。临时更改max_Pages per _rpc请在客户端上运行以下命令:1 client$ 1Lct] set Param osc.fsname-OST* .max pages per Lpc=16M使更改永久生效，运行:1 client$ lctl set Param -P obdfilter.fsname-OST*.osc.max_ pages per rpc=1™!注意OST 的prw_size可以随时更改。但客户端必须重新安厂并重新协商 RPC 最大大小。34.10. 提升 Lustre 小文件 IO 性能应用程序将小文件块从多个客户端写入单个文件可能会导致较送的 IO 性能。提高Lustre 文件系统小文件的 IO 性能，我们可以:。在将 IO 提交到 Lustre 文件系统之前，应用程序先进行 IO 聚合。默认情况下，Lustre 软件将强制执行 POSIX 语义一致性。因此，如果它们都同时写入同一文件会导致客户端节点之间发生 ping-pong 锁定。如果应用程序使用MPI-IO，则实现此功能的一种直接的方法是在 Lustre ADIO 驱动程序中使用MPI-IO CollectiveWrite 功能。418\nayLustre 文件系统操作手册 译mKAs大。 证应用程序对文件执行 4kB 的O_DIRECT大小IO，并禁用输出文件上的锁定。这可以避免部分页面 IO 提交，以及客户端之间的争用。。让应用程序写入连续的数据。。为 OST 添加更多磁盘或"
    ]
}