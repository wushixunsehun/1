{
    "query": "es集群用户统一资源接口服务启动失败，如何检查原因？",
    "summaries": [
        "系统在运行过程中出现错误，提示“ERROR failed to register user buffer datatype”，涉及地址和长度信息，可能与内存或I/O操作有关。随后出现多个UCX错误日志，均指向glex_md.c文件的362行，表明在注册用户缓冲区时发生问题。最后，任务被中止，显示“Aborted”和“STEP 3596459. ON cn1944 CANCELLED AT”，表明作业执行失败，可能与通信库或资源管理器相关。",
        "系统出现多个故障，包括TH-3F的握手次数变化、TH-HPC的raid1和raid2超时故障。集群总览页面整合了节点、作业和存储信息。运维平台用于处理故障，值班人员可通过登录平台查看报警信息并执行操作。Lustre存储故障处理包括挂起作业、查询日志、重启节点等步骤。",
        "文本总结：本文介绍了GlusterFS系统中几种常见故障的处理方法，包括自愈进程、配额进程、服务器连接数减少、Brick不可用等问题。针对每个问题，提供了定位和解决步骤，如使用脚本查找故障进程、重启glusterd服务、检查服务器状态等。此外，还提到某些卷存储使用率超过95%的严重告警情况，并给出初步处理步骤。"
    ],
    "contents": [
        "TH-3F: mn26 : S07C11PU06,，\n\n握手次数发生变化\n\nTH-HPC: ost64 : raid1出现\ntimeout故障\n\n” TH-HPC: ost64 : raid2出现\n\ntimeout故障\n（2）集群总览\nHPC、HPC4、1903都有自己的集群总览页面，将节点情况、作业情况、存储情况集中展示，以TH-HPC4总览页面为例，可以看出其实就是把原来分散的节点、作业、存储使用率监控数据整合到一个页面展示。\n© 2024年05月29日15.35 。 用户名-fengqiang 退出 |\n\nTH-HPCAEIE |\n\nnnil wasecere |)TeI] reuse7\n\neRss© pending 9 ne\n=omm\n\n服务节点o55%所 ee\n2Bs2s加\n\noR加15416127703(T)\n77\n\nseat=pn\n».6 6eo 0 0*\n\nJIL| |__ eee II\nost i7\n\nTT\n三 系统故障处理\n一线值班员通过运维平台处理系统故障，下面介绍运维平台的登录、使用方法。\n3.1 运维平台登录\n每个值班人员都有自己的运维平台账号，值班室调试机的chrome浏览器上有登录运维平台的书签，值班人员点击书签，输入用户名和密码，再点击登录，可登录到运维平台。\n© 新标签页x 十\n\n& > GC Q 在Google中拓索，或者输入一个网址\n\nB ses SO NSCCRERE @ SEEEXHET © EesueTe B 2ARER\n图3-1 浏览器书签\n一一\n\n河统一监控运维平台\n\n一一\n\n用户登录\n图3-2 登录页面\n3.2 功能概述\n登陆运维平台后，选择左侧边栏的 “运维总览”页面，该页面显示当前的系统报警情况，这样值班人员就可以直接在运维平台上获取需要处理的报警信息，不需要去显示系统报警的监控大屏去获取报警信息。\n右上角点击账号--个人信息，可以更改密码。\n统一监控运维平台iQxX * 2 ee\n\nOo RL报警开关\n04\n剧本编排\n剧本执行\n集群故障点故障级别发生时间状态操作\nTH-3F7. =e 警告2024-05-",
        "Left\nVcg/e8/s96 -Not in progress -\n/cO/e8/sl_ -Not in progress -\nVcg/e8/s2 -Not in progress -\n/cQ/e8/s3 -Not in progress -\n/cQ/e8/s4 -Not in progress -\nVcg/e8/s55 -Not in progress -\n/cQ/e8/s6 -Not in progress -\nVcg/e8/s7 -Not in progress -\n/c0/e8/s8 -Not in progress -\nVcg/e8/s59-Not in progress -\n/cQ/e8/s10 -Not in progress -\n/cQ/e8/sl1l1 -Not in progress -\n/c@/e8/s12 -Not in progress -\n3.7.2 自愈进程故障\n某个节点的heal进程发生故障,请首先定位该heal进程.然后重启该节点glusterd服务,知道该服务恢复.\nssh连接到mn1\n# cd /root/tools/gluster\n# ./find_bad_healprocess.sh\n以hl-1b为例,会看到类似如下的输出:\nSelf-heal Daemon on hl1-1bN/AN/AN/A8328\n# ssh hl1-1b\n# systemctl restart glusterd\n3.7.3 配额进程故障\n某个节点的quota进程发生故障,请首先定位该quota进程.然后重启该节点glusterd服务,知道该服务恢复.\nssh连接到mn1\n# cd /root/tools/gluster\n# ./find_bad_quotaprocess.sh\n以hl-1b为例,会看到类似如下的输出:\nQuota Daemon on hl1-1bN/AN/AN/A8281\n# ssh hl1-1b\n# systemctl restart glusterd\n3.7.4 服务器连接数减少\n这种情况一般是由于某个服务器的glusterd服务发生故障导致/宕机,处理流程如下：\n首先定位故障机器:\nssh连接到mn1\n# cd /root/tools/gluster/\n# ./find_bad_peer.sh\nHostname: hl1-2b\nUuid",
        "ERROR failed to register user buffer datatype @x8 address @x4e00ac497010 len 344964: Input/output error\n日\n1\n2\n3\n4\n5\n6\n7\n8\n9\n/th¥s1/software/mpich/mpi-x-gcc1@.2.0/1ib/Libmpi.so.12(PMPI_Recv+0x294) [ex488817815f44]\n/th¥s1/home/wf1iue6/dy /PanguLU-4.1.@/examples/./pangulu_example.elf(+@x16ed8) [@xaaaaeSa49ed8]\n/th¥s1/home/wf1iu6/dy /PanguLU-4.1.@/examples/./pangulu_example.elf(+@x1883@) [@xaaaaeSa4b830]\n18 /thfs1/home/wf1iu@6/dy/PangulU-4.1.@/examples/../pangulu_example.elf(+0x19078) [@xaaaaeSa4c078]\n311 /thfs1/home/wf1iue6/dy/PanguLU-4.1.0/examples/ ./pangulu_example.elf(+0x5334) [@xaaaaeSe38334]\n12 /ths1/home/wf1iue6/dy/PanguLU-4.1.0/examples/./pangulu_example.elf(+0x3@a8) [@xaaaaeSe360a8]\n343 /Lib/aarch64-Linux-gnu/libc.so.6(libc_start_main+@xe8) [0x4¢00172ed090]\n314 /thfs1/home/wf1iue6/dy/PanguLU-4.1.0/examples/./pangulu_example.elf(+0x34b4) [@xaaaaeSe364b4]\n[1727595377.588341] [cn1945:3200030:0]      glex_md.c:362 UCX ERROR GLEX cre\n[1727595377.588557] [cn1945:3260030:0]     glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588608] [cn1945:3200030:0]    glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588639] [cn1945:3200030:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588675] [cn1945:3200030:0]     glex_md.c:",
        "统一监控运维平台iQxX * 2 ee\n\nOo RL报警开关\n04\n剧本编排\n剧本执行\n集群故障点故障级别发生时间状态操作\nTH-3F7. =e 警告2024-05-16T15:33:05未处理\nTH-HPC44e 警告2024-05-16T15:05:41未处理\nTH-3Feeee 通知2024-04-10T16:23:35未处理\nTH-3Mi7e 通知2024-04-04T08:22:06未处理\n\n共4条数据10条[页\n点击左侧边栏的“剧本执行”，可以切换到运维操作页面，点击TH-HPC、TH-3F等可以连接对应的集群，超过5分钟没有操作，将断开连接集群。\n运维操作的主要功能如下图所示：\n统一监控运维平台= 运维管理、\n\n定制大屏Bas 运维总揪\n\n其他操作 节点操作\n\nTH-HPC4\n\nTH-3F\nBIASTH-3M.\n\nTH-3K\n\n操作提示: 点击左侧树中集群名以连接集群 ~ 点击操作类型 ~ 点击操作按钮 ~ 填入参数，执行操作\n\n查看\n文档\n存情节点，怠 。重户、关机、开机、重启pdp、查看负载、查看日志.\n| ESR oO BEE, 查看dmesg、查看lustre active情况、关机、开机\n\n重启ntp\n本\n重启mysql\n\n| BRR © BSRR SHEARER HERRRACAE SRTBE SMa Bie.\n注意：运维操作页面内，在不同集群之间切换，标签保留。如果运维操作切换到运维总览或监控页面，运维操作内的标签全部会关掉。\n3.3 Lustre存储故障\n3.3.1 mds/ost报宕机或报unhealthy\n（1）挂起对应分区作业，并在微信群通知业务部门。\n查询报警的mds/ost属于哪个分区，参照下表：\nmds节点 | ost节点 | 存储分区 | 所属集群\nmds0 | ost0-7,ost40-47 | THL5 | HPC-ES\nmds1 | ost8-39 | THL6 | HPC1\nmds2 | ost48-79 | THL7 | HPC2\nmds3 | ost80-111 | THL8 |",
        "HPC-ES\nmds1 | ost8-39 | THL6 | HPC1\nmds2 | ost48-79 | THL7 | HPC2\nmds3 | ost80-111 | THL8 | HPC3\nmds4 | ost112-143 | fs1 | HPC4\n例如mds1宕机，即需要挂起THL6的分区作业，如下图所示。\n统一监控运维平台= 运维管理、\n\n定制大屏剧本执行\n\nTH-HPC\n其他操作 节点操作\n\n TH-HPCA© TH-HPC > THL6\n© TH-HPC\n日 中 存储分区操作\ngris 2EL分区作业恢复\n\nQTH7\nOTH\nO AiReE\nO 用户操作\n© 作灿操作\n\n四 肥各二人矿\n如下图查看日志，如果有-30或scsi cmnd错误，联系二线值班人员处理；如果没有报-30或scsi cmnd错误，进行下一步。\n统一监控运维平台= 运维管理、\n\n定制大屏剧本执行\n\nTH-HPCTH-HPC4\n\n其他操作\n\nof 节点编号: mds1\n\n日 ce TH-HPC\n序号: 2488\n©) HPC1-127\n日 storage节点名称: mds1\n TH-3F\n\n查询内存\n\n清除进程标记硬盘\n\n所属集群 TH-HPC\n所属分区:_null\n\n存储位置: 老机房-TH-HPC-HPC1-\n127-21.0\n\n查询硬盘信息Airaid (SB\n\ncpu进程排序mem进程排序\n\n硬盘大小. 无硬盘\n节点状态: 连接成功 |\n\n查询rsf信息\n\nBRE\n重启mds。选择“其他操作”—对应集群—“其他操作”—“电源管理”。\n输入“节点名”和“动作（重启）”后确认。\nTH-HPC TH-HPC4\n节点操作\n\nTH-HPC4PDTH-HPC\n\nafer]\n\n剧本编排BO 存储分区操作\n\nOTHLS登陆节点部署客户端-， MDS节点部署客户.， OSTHRBBEP...计算节点部署客户端.， 远程在线用户\n剧本执行四THL6\n二emsiveenee wm—\n© 资源操作\n\n0 用户操作\n\n© 作业操作mds1:查询日志 久",
        ":3200030:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588675] [cn1945:3200030:0]     glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588722] [cn1945:3200030:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588758] [cn1945:3200030:0]     glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.680342] [cn1945:3200043:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.680526] [cn1945:3200043:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.680558] [cn1945:3200043:0]     glex_md.c:362 UCX ERROR GLEX cre\n[1727595377 680586] [cn1945:3200043:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377 680609] [cn1945:3200043:0]     glex_md.c:362 UCX ERROR GLEX cre\n[1727595377.680647] [cn1945:3200043:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.680671] [cn1945:3200043:0]     glex_md.c:362 UCX ERROR GLEX cre:\nyhru\nslurmstepd: error:\ncn1945: task 3: Aborted\nmpi/pmix_v3: _errhandler: cn1945 [1]: pmixp_client_v2.\nerror:\n2210:\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nError handler invoked:\nslurmstepd: error: *** STEP 3596459. ON cn1944 CANCELLED AT",
        "宕机,处理流程如下：\n首先定位故障机器:\nssh连接到mn1\n# cd /root/tools/gluster/\n# ./find_bad_peer.sh\nHostname: hl1-2b\nUuid: 9068d4b9-c4cd-4c81-9d83-00ed035f4596\nState: Peer in Cluster (Disconnect)\n然后确定故障类型并解决\n# ssh hl1-2bhostname\n如果能正常输出主机名,则该节点glusterd服务导致,连接到该服务器重启glusterd服务即可\n#ssh hl1-2b\n# systemctl restart glusterd\n如无法输出主机名\n#ping hl1-2b 查看机器状态 如无法ping通该机器死机状态\n# cnpower hl1-2b reset 重启机器即可\n3.7.5 Brick不可用\n某个节点的brick进程发生故障,请首先定位该brick进程\n定位方法如下:\nssh连接到mn1\n# cd /root/tools/gluster/\n# ./find_bad_brickprocess.sh\nBrick 121.16.201.4:/data/TH-NS/sda1/brick491520N/A8258\n定位到之后, 首先确定是否为硬盘故障,以如上所示为例:\n# ssh 121.16.201.4\n# cd /data/TH-NS/sda1/\n# touch tmpfile\n如果没有Read-Only 或者 Input/output error报错,则请重启glusterd服务\n# systemctl restart glusterd\n如果有 同一台机器4个同时报错,请查看下服务器是否宕机,如是宕机,请重启该服务器\n#grep 121.16.201.4 /etc/hosts\n121.16.201.4 hl1-2b\n# cnpower hl1-2b reset\n等待机器恢复即可\n3.8 xx卷存储使用率大于95%\nost53THL7-0ST000a卷存储使用率大于959TH-HPC存储节点服务。 严重\n\nost57THL7-0SsT0012卷存储使用率大于959TH-HPC存储节点服务。 严重\n\nost67THL7-0sT0026卷存储使用率大于959TH-HPC存储节点服务。 严重\nssh ost95\nmkdir /mnt/ldiskfs\nmount -t ldiskfs /dev/sda /mnt/ldiskfs/\ncd /mnt/ldiskfs/O/0/\nfori in `seq 0 31`;do echo d$i;ll"
    ]
}