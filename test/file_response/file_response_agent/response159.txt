{
    "query": "如何为hpc2系统的qixiang账户的qixiang分区添加节点？",
    "summaries": [
        "该文本介绍了网络分区功能的配置方法和原理。网络分区通过配置不同的partition_key实现节点隔离，使用192位寄存器存储16项分区信息，每个VP通过4位索引值进行访问。分区最小单位为一块cpm板的8个节点。配置方法包括使用脚本对已启动节点进行隔离或关闭分区，以及对未启动节点修改配置文件并集成到启动镜像中。服务器分区需覆盖大部分节点以实现有效隔离。",
        "天津新系统采用JBOD固定连接方式，需获取HBA卡的PCI ID并为每张卡的PORT设置别名。配置文件`/etc/zfs/vdev_id.conf`定义了HBA卡与JBOD的连接关系，确保硬盘命名一致。多路径配置需通过`multipath.conf`实现，使用WWID和硬盘别名进行映射。系统提供工具`vdev`自动生成`vdev_id.conf`，依赖Python、lsscsi和sg_utils，通过`jbod.json`配置文件定义JBOD名称与WWN对应关系。配置完成后需执行命令使配置生效并检查设备数量是否一致。",
        "HPC4 gpu分区支持单节点双卡和八卡配置，建议一个节点提交两个作业以避免资源浪费。未指定设备号时，可通过CUDA_VISIBLE_DEVICES设置GPU编号；程序中指定设备号时，无需额外设置。PyTorch和TensorFlow的设备指定方法可参考相关链接。"
    ],
    "contents": [
        "【已解决】HPC4 gpu分区单节点提交两个作业\n**标签**: gpu\n**创建时间**: 2022-06-30 15:22:52\n**更新时间**: 2022-06-30 15:22:52\n**作者**: 杜思慧\n**1.背景**\n目前hpc4上的gpu分区配置为单节点双卡，gpu1分区为单节点八卡，可mix使用；\n在gpu分区为避免浪费，建议一个节点提交两个作业\n**2.脚本**\n未在程序中指定设备号时：\n#!/bin/bash\nmodule add pytorch/1.11.0-cu11.3-py3.9\nmodule add loginnode/ln0\nCUDA_VISIBLE_DEVICES=0 python 3d.py &\nCUDA_VISIBLE_DEVICES=1 python 3d-1.py &\nwait\n在程序中指定设备号时：\n#!/bin/bash\nmodule add pytorch/1.11.0-cu11.3-py3.9\nmodule add loginnode/ln0\npython 3d.py &\npython 3d-1.py &\nwait\n**3.备注**\n程序中指定设备号的方法：\nPytorch: https://www.cnblogs.com/darkknightzh/p/6836568.html\nTensorflow: https://blog.csdn.net/weixin_31866177/article/details/89403727",
        "3.6.1、说明\nvdev_id.conf 配置文件生成工具名为： vdev。\n依赖于：\n- python2.7\n- lsscsi\n- sg_utils\n以上三个依赖都已经被安装在标准的 linux 发行版中，无需额外安装。\nvdev 本质上是一个 python 脚本，通过 sg_ses 命令读取/sys/class/enclosure 下每条 scsi 链路中的硬盘信息， 包括硬盘槽位和硬盘的 wwn 编码，然后按照 vdev_id.conf 配置文件格式生成所需的配置文件。默认在当前目录（PWD）下生成临时配置文件： vdev_id.conf.swp。\n3.6.2、获取 vdev\n下载链接： [ftp://202.197.8.89/stargazer/vdev](ftp://202.197.8.89/stargazer/vdev)\n3.6.3、使用方法\n- 编写 JBOD 配置文件\n具体编写方法请查看本章第二节 jbod.json\n// 按照上文jbod.json中的方式编辑config/jbod.json\n# vim jbod.json\n{\n\"0x5000ccab04109380\": \"JBOD0\",\n\"0x5000ccab04109600\": \"JBOD1\",\n\"0x5000ccab0410b800\": \"JBOD2\",\n\"0x5000ccab04109580\": \"JBOD3\",\n\"0x5000ccab04090800\": \"JBOD4\",\n\"0x500304801f64de3f\": \"JBOD5-F\",\n\"0x5003048017bafe7f\": \"JBOD5-R\"\n}\n- 执行命令生成 vdev_id.conf 配置文件\n不生成 vdev_id.conf 配置文件，仅仅打印配置信息\n# ./vdev print_vdev -c <jbod.json配置文件的路径>\n示例：\n# ./vdev print_vdev -c /opt/stargazer_storage/config/jbod.json\nJBOD5-F:\nalias JBOD5-F-S5 /dev/disk/by-id/wwn-0x5000cca2672c5648\nalias JBOD5-F-S6 /dev/disk/by-id/wwn-0x5000cca26725d1f4\nalias JBOD5-F-S7 /dev/disk/by-id/wwn-0x5000cca2672aa02c\nJBOD5-R:\nalias JBOD5-R-S1 /dev/disk/by-id/wwn-0x5000cca2672c22f8\nalias JBOD5-R-S2 /",
        "JBOD的固定连接方式。天津新系统使用该配置文件。</span>\n3.4.1、说明\n需要获取HBA卡的PCI ID，然后对每张卡的PORT设置别名。\n3.4.2、获取HBA卡的PCI ID\n# lspci | grep LSI\n3b:00.0 Serial Attached SCSI controller: Broadcom / LSI SAS3408 Fusion-MPT Tri-Mode I/O Controller Chip (IOC) (rev 01)\n5e:00.0 Serial Attached SCSI controller: Broadcom / LSI SAS3408 Fusion-MPT Tri-Mode I/O Controller Chip (IOC) (rev 01)\n按照顺序，第一张卡的PCI ID是 **3b:00.0**，第二张卡的PCI ID是 **5e:00.0**。\n> <span style=\"color: red\">注意： 天津新系统固定连接方式中，一组oss和一组JBOD互联，按照数字编号，偶数位的oss的第一张HBA卡（3b）连接第一台JBOD（偶数位编号）的A控，第二张HBA卡（5e）连接第二台JBOD（奇数位编号）的B控；然后奇数位的oss正好相反，奇数位的oss的第一张HBA卡（3b）连接第二台JBOD（奇数位编号）的A控，第二张HBA卡（5e）连接第一台JBOD（偶数位编号）的B控。所以一组OSS和JBOD中，两台OSS的HBA连接的JBOD正好相反。</span>\n3.4.3、配置文件格式\n# cat /etc/zfs/vdev_id.conf\nmultipath\tno\ntopology\tsas_direct\nphys_per_port\t4\n# Additionally create /dev/by-enclosure/ symlinks for enclosure devices\nenclosure_symlinks\tyes\n#\t\tPCI_ID\tHBA\tPORT\tCHANNEL NAME\nchannel 3b:00.0\t0\t\t\tJBODX-S\nchannel 3b:00.0\t1\t\t\tJBODX-S\nchannel 5e:00.0\t0\t\t\tJBODY-S\nchannel 5e:00.0\t1\t\t\tJBODY-S\n每张卡的两个port对应同一个JBOD，所有CHANNEL NAME应该是一样的，",
        "例子：\n[root@localhost flash]# ./znr_read_flash_version.sh © swmge\n0215\n\nyersion check pass\n\nHigh Speed Network\n\n256\n\nTHPCS\n\n15: SWMO9_ZNRO\n3.3.4 分区配置\n3.3.4.1 基本原理\n网络分区功能主要是从网络方面通过对需要划分的节点和服务器配置不同的partition_key进行隔离；芯片设计了3个分区信息表配置寄存器共192位，包含16项分区信息，每个分区信息为12位；使用分区信息索引配置寄存器进行索引，每个VP使用4位分区信息索引值对16项分区信息进行索引。4个分区信息索引配置寄存器共256位，包含64项（每个VP使用1项）分区信息索引值，每个分区信息索引值为4位。\n注意，由于cpm板上8个点为立方体结构，路由会经过中间“过路”节点，因此分区功能最小以一块cpm板8个节点为单位进行。\n3.3.4.2 具体示例\n分区目标\n将P0-P19/ION[0-59]/mn[0-8]/ln[0-7]与其他的计算柜/ION/mn/ln隔离开来，进行分区。\n分区配置方法\n1）对已正常起来的节点或服务器\n通过/home/test641/tfq/shelltools_zni 下的脚本配置。\n./set_nodes_partition.shnodelistpartition_mask(0x801/0x802)。\n把隔离的两部分节点分别配不同的partition_mask，可实现节点隔离（互相不通）。\n若要关闭分区隔离功能，可使用脚本完成配置：./close_nodes_partition.sh nodelist。\n2）对未起来的节点或重启的节点\n根据分区隔离分界的节点id进行判断，修改/home/test641/tfq/shelltools_zni下zninet_cpm文件中如图所示的标注位置的值；然后把此修改的zninet_cpm(需要覆盖/etc/init.d/下的zninet)和set_partition.sh/close_partition.sh(需要复制到/etc/下)交给651做到节点拉核启动镜像中，分区功能在节点拉核起驱动过程中就生效了，后期不需要单独再配置。\n3）服务器分区功能配置\nmn",
        "JBODX-S\nchannel 5e:00.0\t0\t\t\tJBODY-S\nchannel 5e:00.0\t1\t\t\tJBODY-S\n每张卡的两个port对应同一个JBOD，所有CHANNEL NAME应该是一样的，为了保证硬盘的命名格式是JBODX-SX，所以CHANNEL NAME命名为JBODX-S。\n3.4.4、配置生效\n# udevadm trigger\n3.4.5、检查\n执行以下命令获取所有硬盘的数量，该数量应该与所有实际硬盘数量一致。\n# ls /dev/disk/by-vdev | grep -v part | wc -l\n3.5、多路径 - multipath.conf\n3.5.1、说明\n需要对每一块硬盘进行多路径映射而不是以前那种已经做好的 RAID。\n3.5.2、配置格式\n配置格式与以往的多路径配置方法相同。也是需要 **wwid** 和 硬盘别名\nmultipath {\nwwid wwid编号\nalias 硬盘别名\n}\n硬盘别名命名方式和 **vdev_id.conf** 相同，即 **JBODX-F|R-SX**。具体说明请查看上一节。\n配置文件示例：\n##################### JBOD0 ###################\nmultipath {\nwwid    5000cca291493ad0\nalias   JBOD0-S0\n}\nmultipath {\nwwid    5000cca2914a88b8\nalias   JBOD0-S1\n}\nmultipath {\nwwid    5000cca2912efc74\nalias   JBOD0-S2\n}\n3.5.3、生成映射\n将生成的配置文件拷贝到 **/etc** 下， 并重命名为 **multipath.conf** 执行以下命令生成映射：\n# multipath -v2\n# multipath -v3\n执行完毕后请检查 **/dev/mapper** 下是否生成对应的硬盘链接，检查数量是否和硬盘数一致。或者使用一下命令查看多路径信息。\n# multipath -ll\n3.6、vdev_id.conf 配置文件生成工具\n3.6.1、说明\nvdev_id.conf 配置文件生成工具名为： vdev。\n依赖于：\n- python2.7\n- lsscsi\n- sg_utils\n以上三个依赖都已经被安装在标准的"
    ]
}