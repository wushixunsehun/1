{
    "query": " 在使用 Intel MPI 编译的 CP2K 程序进行 AIMD 模拟时，作业频繁因信号 9 终止，堆栈跟踪显示故障发生在MKLMPI_Bcast()调用阶段，且涉及 UCX 库和 PMIx 进程引擎错误。请分析可能的故障原因并给出对应的解决方案。",
    "summaries": [
        "该文本为程序崩溃的堆栈跟踪，显示在 `MKLMPI_Bcast()` 处卡住，涉及 MPI 通信和 UCX 库。问题可能与 MKL 使用的 BLACS 库和 Intel MPI 的兼容性有关。建议更换为 OpenMPI 编译以解决问题。堆栈中还涉及多个线程的调用链，包括 UCX、libevent、pthread 和 MPI 函数。核心问题是 MPI 广播操作阻塞，可能由内存访问错误或库版本不兼容引起。",
        "该日志显示MPI作业在运行过程中出现错误，主要原因是`MPI_File_set_errhandler`调用失败，错误类型为无效参数，且错误处理程序不是文件错误处理程序。多个节点报告相同错误，导致作业被取消。目前可用环境为mpich/4.0.2-mpi-x-gcc10.2.0，性能较HPC系统慢3.28倍，属于正常范围。部分组合如3m gcc+openmpi和ex gcc+openmpi会出现内存不足或MPI发送错误。建议在ex系统使用debug版本的MPI库进行深入测试，并设置UCX日志级别为WARN。",
        "CP2K计算在AIMD模拟中卡住，停留在新一步的SCF迭代。通过查看日志发现使用了7个DIIS向量，且CPU使用率接近100%，内存占用较高。进程cp2k.popt在多个线程中运行，CPU占用率高达106.7%。检查系统负载显示为56.16，表明计算任务非常密集。通过pstack查看进程堆栈，发现其在epoll_wait中等待，可能与MPI或网络通信有关。"
    ],
    "contents": [
        "in comm 0): Fatal error in internal_File_set_errhandler: Invalid argument, error stack:\nyhrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\n‘internal_File_set_errhandler(86): MPI_File_set_errhandler(MPI_FILE_NULL, errh=0x94000000) failed\ninternal_File_set_errhandler(62): Error handler is not a file error handler\nslurmstepd: error: *** STEP 32333.0 ON cn10305 CANCELLED AT 2023-02-22T09:45:32 **x\nAbort(671707404) on node 153 (rank 153 in comm 0): Fatal error in internal_File_set_errhandler: Invalid argument, error stack:\ninternal_File_set_errhandler(86): MPI_File_set_errhandler(MPI_FILE_NULL, errh=0x94000000) failed\ninternal_File_set_errhandler(62): Error handler is not a file error handler\nAbort(671707404) on node 69 (rank 69 in comm @): Fatal error in internal_File_set_errhandler: Invalid argument, error stack:\ninternal_File_set_errhandler(86): MPI_File_set_errhandler(MPI_FILE NULL, errh=0x94000000) failed\ninternal_File_set_errhandler(62): Error handler is not a file error handler\nAbort(671707404) on node 55 (rank 55 in comm @): Fatal error in internal_File_set_errhandler: Invalid argument, error stack:\ninternal_File_set_errhandler(86): MPI_File_set_errhandler(MPI_FILE_NULL, errh=0x94000000) failed\ninternal_File_set_errhandler(62): Error handler is not a file error handler\n结论\n目前可以",
        "/intel64_lin/libimf.so (0x00001511bf850000)\nlibintlc.so.5 => /fs2/software/intel/2019.4/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin/libintlc.so.5 (0x00001511bf5de000)\nlibsvml.so => /fs2/software/intel/2019.4/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin/libsvml.so (0x00001511bdc3a000)\nlibirng.so => /fs2/software/intel/2019.4/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin/libirng.so (0x00001511bd8c8000)\n/lib64/ld-linux-x86-64.so.2 (0x00001511c3388000)\nlibcrypto.so.1.1 => /lib64/libcrypto.so.1.1 (0x00001511bd3df000)\nCP2K计算AIMD卡住\n卡在新一步的scf\n$ tail -f cp2k.out\nusing   7 DIIS vectors\nsafer DIIS on\nPreconditioner : FULL_ALL            : diagonalization, state selective\nPrecond_solver : DEFAULT\nstepsize       :    0.15000000                  energy_gap     :    0.08000000\neps_taylor     :   0.10000E-15                  max_taylor     :             4\nOT\nStep     Update method      Time    Convergence         Total energy    Change\n进入计算节点\n$ top\ntop - 16:40:36 up 9 days,  9:20,  2 users,  load average: 56.16, 56.06, 56.02\nTasks:  62 total,  57 running,   5 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 99.5",
        "56.06, 56.02\nTasks:  62 total,  57 running,   5 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 99.5 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.5 hi,  0.0 si,  0.0 st\nMiB Mem : 257075.8 total, 226431.3 free,  28400.1 used,   2244.4 buff/cache\nMiB Swap:      0.0 total,      0.0 free,      0.0 used. 225470.1 avail Mem\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n139745 liudj     20   0 1127136 495660 103280 R 106.7   0.2 142:14.94 cp2k.popt\n139746 liudj     20   0 1165844 527248 103596 R 106.7   0.2 142:13.08 cp2k.popt\n139765 liudj     20   0 1264248 620192 103528 R 106.7   0.2 142:11.14 cp2k.popt\n139768 liudj     20   0 1137360 489852 103780 R 106.7   0.2 142:52.89 cp2k.popt\n139719 liudj     20   0 1237952 604376 103408 R 100.0   0.2 142:03.62 cp2k.popt\n查看第一个PID\n$ pstack 139745\nThread 3 (Thread 0x14d65cb25700 (LWP 139836)):\n#0  0x000014d6659dda07 in epoll_wait () from /lib64/libc.so.6\n#1  0x000014d6641614d0 in ucs_event_set_wait () from /usr/local/mpi-intel/ucx/lib/libucs.so.0\n#2  0x000014d66413c27e in ?? () from /usr",
        "_base (matrix=0x14d65cc38570 <_glex_dma_ep_send_mp>, eigenvectors=<error reading variable: Location address is not set.>, eigenvalues=<error reading variable: Cannot access memory at address 0x794>, info=<error reading variable: Cannot access memory at address 0x0>) at /fs2/home/liudj/nscc/cp2k/cp2k-2022.2/src/fm/cp_fm_diag.F:544\n#21 0x0000000002d0ca5c in cp_fm_diag::cp_fm_syevd (matrix=0x14d65cc38570 <_glex_dma_ep_send_mp>, eigenvectors=<error reading variable: Location address is not set.>, eigenvalues=<error reading variable: Cannot access memory at address 0x794>, info=<error reading variable: Cannot access memory at address 0x0>) at /fs2/home/liudj/nscc/cp2k/cp2k-2022.2/src/fm/cp_fm_diag.F:387\n#22 0x0000000002d0c341 in cp_fm_diag::choose_eigv_solver (matrix=0x14d65cc38570 <_glex_dma_ep_send_mp>, eigenvectors=<error reading variable: Location address is not set.>, eigenvalues=<error reading variable: Cannot access memory at address 0x794>, info=<error reading variable: Cannot access memory at address 0x0>) at /fs2/home/liudj/nscc/cp2k/cp2k-2022.2/src/fm/cp_fm_diag.F:190\n卡在 MKLMPI_Bcast ()\nMKL 使用的blacs库对应的intelmpi，更换openmpi编译解决",
        ".so.40 (0x00001511c278d000)\nlibm.so.6 => /lib64/libm.so.6 (0x00001511c240b000)\nlibiomp5.so => /fs2/software/python/3.8_anaconda_2021.05/lib/libiomp5.so (0x00001511c1ff4000)\nlibpthread.so.0 => /lib64/libpthread.so.0 (0x00001511c1dd4000)\nlibdl.so.2 => /lib64/libdl.so.2 (0x00001511c1bd0000)\nlibc.so.6 => /lib64/libc.so.6 (0x00001511c180b000)\nlibgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00001511c15f3000)\nlibopen-rte.so.40 => /fs2/software/openmpi/4.1.4-mpi-x-icc19.0/lib/libopen-rte.so.40 (0x00001511c132c000)\nlibopen-pal.so.40 => /fs2/software/openmpi/4.1.4-mpi-x-icc19.0/lib/libopen-pal.so.40 (0x00001511c1062000)\nlibrt.so.1 => /lib64/librt.so.1 (0x00001511c0e5a000)\nlibutil.so.1 => /lib64/libutil.so.1 (0x00001511c0c56000)\nlibz.so.1 => /lib64/libz.so.1 (0x00001511c0a3f000)\nlibhwloc.so.15 => /lib64/libhwloc.so.15 (0x00001511c07ef000)\nlibevent_core-2.1.so.6 => /lib64/libevent_core-2.1.so.6 (0x00001511c05b6000)\nlibevent_pthreads-2.1.so.6 => /lib64/libevent_pthreads-2.1.so.6 (0x00001511c03b3000)\nlibifport.so.5 => /fs2/software/intel/2019.4/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin/libifport.so.5 (0x00001511c0185000)\nlibifcoremt.so.5 => /fs2/software/intel/2019.4/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin/libifcoremt.so.5 (0x00001511bfdf0000)\nlibimf.so => /fs2/software/intel/2019.4/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin/libimf.so (0x00001511bf850000)\nlibintlc.so.5 => /fs2/software/intel/2019.4/compilers_and_libraries_2019.4.243/linux/compiler/lib/intel64_lin/libintlc",
        "usr/local/mpi-intel/ucx/lib/ucx/libuct_glex.so.0\n#4  0x000014d6646231cc in ucp_worker_progress () from /usr/local/mpi-intel/ucx/lib/libucp.so.0\n#5  0x000014d666aa7cf2 in MPIR_Wait_state () from /fs2/software/mpich/4.0.2-mpi-x-icc19.0/lib/libmpi.so.12\n#6  0x000014d666a5baa9 in MPIC_Recv () from /fs2/software/mpich/4.0.2-mpi-x-icc19.0/lib/libmpi.so.12\n#7  0x000014d66698601b in MPII_Scatter_for_bcast () from /fs2/software/mpich/4.0.2-mpi-x-icc19.0/lib/libmpi.so.12\n#8  0x000014d6669876e5 in MPIR_Bcast_intra_scatter_ring_allgather () from /fs2/software/mpich/4.0.2-mpi-x-icc19.0/lib/libmpi.so.12\n#9  0x000014d666a12582 in MPIR_Bcast () from /fs2/software/mpich/4.0.2-mpi-x-icc19.0/lib/libmpi.so.12\n#10 0x000014d66684d3af in PMPI_Bcast () from /fs2/software/mpich/4.0.2-mpi-x-icc19.0/lib/libmpi.so.12\n#11 0x0000000008312fef in MKLMPI_Bcast ()\n#12 0x00000000082fd5de in dgebr2d_ ()\n#13 0x00000000031e0bf1 in pdlaed3_ ()\n#14 0x00000000031dd6ef in pdlaed1_ ()\n#15 0x00000000031dcfb1 in pdlaed0_ ()\n#16 0x0000000003145899 in pdstedc_ ()\n#17 0x00000000030c3ad4 in mkl_pdsyevd0_ ()\n#18 0x00000000030c28e4 in mkl_pdsyevdm_ ()\n#19 0x00000000030c1b89 in pdsyevd_ ()\n#20 0x0000000002d0d12e in cp_fm_diag::cp_fm_syevd_base (matrix=0x14d65cc38570 <_glex_dma_ep_send_mp>, eigenvectors=<error reading variable: Location address is not set.>, eigenvalues=<error",
        "set_errhandler(MPI_FILE_NULL, errh=0x94000000) failed\ninternal_File_set_errhandler(62): Error handler is not a file error handler\n结论\n目前可以用的环境是mpich/4.0.2-mpi-x-gcc10.2.0，GCC/10.2.0\n性能方面迭代100次用了1小时22分钟，相比我们测试的HPC系统100次迭代用了25分钟，慢了3.28倍，属于正常范围内。\n这个算例1000E-15的模拟我在国产系统和hpc两边都同时测试下\n遗留问题\n还存在几种组合会出现问题\n3m gcc+openmpi 会报OUT OF MEMOREY\nex gcc+mpich   会有mpiisend类的报错\nex gcc+openmpi 会报OUT OF MEMOREY\n深入测试\n在ex系统使用debug版本的mpi库\nexport UCX_LOG_LEVEL=WARN\nmodule add MPI/mpich/4.0.2-mpi-x-dbg-icc19.0",
        "in ucs_event_set_wait () from /usr/local/mpi-intel/ucx/lib/libucs.so.0\n#2  0x000014d66413c27e in ?? () from /usr/local/mpi-intel/ucx/lib/libucs.so.0\n#3  0x000014d665e7f1cf in start_thread () from /lib64/libpthread.so.0\n#4  0x000014d6658e7dd3 in clone () from /lib64/libc.so.6\nThread 2 (Thread 0x14d65e059700 (LWP 139780)):\n#0  0x000014d6659dda07 in epoll_wait () from /lib64/libc.so.6\n#1  0x000014d664d384a9 in epoll_dispatch () from /usr/lib64/libevent_core-2.1.so.6\n#2  0x000014d664d2e188 in event_base_loop () from /usr/lib64/libevent_core-2.1.so.6\n#3  0x000014d6654004c6 in progress_engine () from /usr/lib64/libpmix.so.2\n#4  0x000014d665e7f1cf in start_thread () from /lib64/libpthread.so.0\n#5  0x000014d6658e7dd3 in clone () from /lib64/libc.so.6\nThread 1 (Thread 0x14d66abd9c80 (LWP 139745)):\n#0  0x000014d65cc37ffd in _glex_check_mpq_pending () from /usr/local/mpi-intel/ucx/lib/ucx/libuct_glex.so.0\n#1  0x000014d65cc38d85 in glex_probe_next_mp () from /usr/local/mpi-intel/ucx/lib/ucx/libuct_glex.so.0\n#2  0x000014d65cc32cde in uct_glex_probe_mp () from /usr/local/mpi-intel/ucx/lib/ucx/libuct_glex.so.0\n#3  0x000014d65cc2f2d1 in uct_glex_progress () from /usr/local/mpi-intel/ucx/lib/ucx/libuct_glex.so.0\n#4  0x000014d6646231cc in ucp_worker_progress () from /usr/local/mpi-intel/ucx/"
    ]
}