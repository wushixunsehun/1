{
    "query": "用户发现存储目录/fs2的磁盘配额已超软限制（512G），系统提示“grace: 6d23h”。请说明此时应采取的措施及监控命令。",
    "summaries": [
        "为使用 ldiskfs 格式的 OST 指定非默认的 inode ratio 可能导致索引节点总数超过限制，从而引发空间超限错误，浪费空间并降低 e2fsck 速度。应使用默认 inode ratio 以确保系统正常运行。OST 文件系统检查时间受多种因素影响，正常情况下每 TiB 需 5-30 分钟，若存在大量错误则时间会增加。Lustre 文件系统有多个极限值，如最大 MDTs 数量、OSTs 数量、OST 大小、客户端数量等，这些值受架构和系统限制，部分可通过重新编译修改。文件条带化、文件大小、目录文件数等也有限制，具体数值因文件系统类型（如 ldiskfs 或 ZFS）而异。Lustre 支持大文件和大量文件，但实际容量受限于 OST 空间和配置。",
        "问题描述：在将数据从HPC系统迁移到3F时，发现使用`du`命令统计的文件大小不同。原因在于不同系统对磁盘占用空间的计算方式不同。解决方法是使用`du -b`命令，该命令以字节为单位统计文件的实际大小，而非磁盘占用空间，从而确保不同系统间结果一致。`du -b`等价于`du apparent-size block-size=1`，能更准确地反映文件真实大小。",
        "本文档介绍了TH-eX系统的用户分区设置、权限限制、磁盘配额以及状态查看命令。用户根据不同的分区有相应的结点数和任务运行时间限制。系统还对用户权限进行管理，基于合同规模限制使用资源，并要求用户在申请资源后才能访问计算结点。磁盘配额方面，用户有存储和文件数量的软硬限制，超出限制将影响数据操作。用户可通过相关命令查看分区、结点和作业状态，确保合理使用系统资源。"
    ],
    "contents": [
        "有具体如下表所示:表 3-1 用户分区设置分区限制ane ja |最多结点数 | BERK 任务最长运行时间debug4 用户调试分区 | 2 | 112 30 分钟oe 包机时用户分区 无short4 包规模普通用户分 HUIS LRT 2Klong4 包规模长队列用户分区 10 天debug6 用户调试分区 | -on 包机时用户分long6 包规模长队列用户分区由账吕权限决定 2 天21\nHISEEtee TH-eX 系统用户手册用户可以使用“大-1”或“yhcontrol show partition partition name” fii, F到相应的分区的详细信息。注意:由于大型集群系统具备一定故障率，为了保证系统稳定性，分区中有限定任务执行时间的限制，因此建议用户为程序设立“断点”从而保证任务由于意外中断后，可以继续运算。3.1.2 用户权限限制除了上述的分区限制，目前还根据用户的申请情况，针对用户做了一定的限制，该限制主要基于用户和中心签订合同的规模。包括: 最多可以使用的结点数、最多可以使用的核数、单个任务最多可以使用的结点数、单个任务最多可以使用的核数等。通过命令“yhacctmgr list association”可查看自己账号的具体权限设置。用户只有查看自己账号的权限，无查询其他账号的权限。用户在使用过程中，如果有超出自己合同范围内的计算规模的计算需求，请基于自己的需求，向中心提出申请，中心会根据用户需要审查后，进行一定的修改。为了保证系统和用户数据的安全，目前普通用户不能在没有申请资源时，就ssh 链接到计算结点，只有分配了相应的计算结点资源后，才能 ssh 到指定计算结点。3.1.3 磁盘配额限制为了合理利用有限的存储资源，目前中心对用户款认进行存储软限制 512G,存储便限制 IT，文件数软限制 100 万，文件数便限制 200 万的磁盘配额限制。用户登录后会出现如图 3-1 的磁盘配额信息:Filesystem used quota Limit grace files quota kimt = grace/fs2 3616 045126 1T - 13942 1999966",
        "上的单个文件大小最大为 16 TiB。在 64 位系统上，这个限制不存在。因此，如采后备文件系统可以文持足够大的对象或者文件很稀蕊，则文件大小可以是2 * 63位 〈8EiB)。单个文件最多可以有 2000 个条市，这使得 64 位 ldiskfs 系统的单个文件能达到 31.25 PiB。的容量文件中可存储的实际数据量取决于文件条市化所在的 OST 中的可用空间量。Lustre 软件使用 ldiskfs 哈希目录代码，依赖于文件名长度，一个目录下最多能包含大约一千万个文件。子目录与闻规文件相同。(在 Lustre 2.8中引入) ，注意从 Lustre2.8 开始，可通过1fs mkdir -c命令将多个 MDTS 上的单个目录条带化来突破此限制，使用多少目录条市数则该最大文件或子目录数量就可以增加多少倍。Lustre55\nLustre 文件系统操作手册详这aX名称 值文件系统上 40 亿/MDT最大文件数 (ldiskfs)，量 256 万亿/MDT(ZFS)最长文件名 255 bytes最长路径名 4096 bytesLustre 文 无限制件系统上当前打开的文件最大数量注意描述文件系统已测试了单个目录下 1000 万个文件。Idiskfs 文件系统的上限为 40 亿个 inodes。默认情况下，MDT 文件系统为每个 node 格式化 2KB空间，即每1TiB MDT 空间有 5.12 亿个 inode。这可以在MDT 文件系统创建时进行初始化。ZFS OVE RANT ACA S| Rk, FE MDT 空间LATER SITAR. ES RG RARE大约 4KiB 的镜像空间，具体取决于配置。每个附加的 MDT 都可容纳上述最大数量的附加文件，这取雇于文件系统中的可用空间以及分布目录和文件。包括底层文件系统在内，单个文件名的最大限制W255 Fo受 Linux VFS 限制，最长路径名为 4096 字HeWoLustre 软件对打开的文件数量疫有限制，但实际上，它还是受制于于 MDS 上的内存大小。MDS 上没有所谓当前打开文件的\" SUR\",为它们只与给定客户端的接口相链接。每个客户端进程最多能打开几王个文件，这取决于它的ulimit。默认情况下，ldiskfs",
        "的状态提示VY. wt(EIS:有的时候用户登录会出现错误提示“Some errors happened when getting quotainfo. Some devices may be not working or deactivated. The data in \"[]\" is inaccurate. ”这是因为登陆结点 quota RAIA lakh, SPH AS BREA EL ae HH用户可以用命令“jlfs quota -g groupname /fs2” KAN BAB CAN EAE AR.或通过命令“lf quota -u username /fs2 ”查看 user 的配额信息。 (其中，groupname 和 username 可以用过 id 命令获得。)3. 2 状态查看命令在用户提交作业前，应先查看系统的使用情况，这样利于用户根据系统使用情况，进行选择。3.2.1 结点状态查看 yhinfo 或 yhiyhi 为 yhinfo 命令的简写，用户可以使用 yhi 或者 yhinfo 命令查看结点的使用情况，从而根据情况做出选择。可以通过命令 whi -1 获得结点更为详细的信息。He 3-3 yhi 输出的关键词说明KE 含义PARTITION 用户可用的计算分区AVAIL 可用状态: up 表示可用; down 表示不可用TIMELIMIT 该分区的作业最大运行时长限制NODES 结点数量4down: 不可用状态idle: 空闲状态alloc: 被分配状态STAT24\nNSz TH-eX 系统用户手册CD: 成功结束，completedF: 失败结束，failedTD: 超时，timeoutNF: 因节点故障而运行失败，node_fail作业状态转换的详细图如下，由于 CD, CA, F 这三个作业状态持续时间很短，因此使用 yhd 命令可能会观察不到这些状态。作业提交用户可以使用 yhg 查看自己提交的作业，为了保证用户的数据安全，普通用户通过 yho 只能看到自己提交的作业。查看作业明细:用户可以通过如下命令来查看目己提交的作业明细其中jobid 表示作业的记号，用户根据目己作业的情况填入即可，之后用户即可以看到该作业十分详细的信息。注意: 用户作业如果长时间为 CG 状态，表示作业没有正常退出，系统管理员",
        "--mkfsoptions=\"-i $((8192 *1024))\" …注意使用 ldiskfs 格式化的 OST 不能超过最多 3.2 (LPR. 401 ESI. AKAOST 指定一个非彰小的 inode ratio，因而导致索引节点总数超出最大值，将导致过早地出现空间超限错误，OST 空间不能被完全使用，浪费空间，使 e2fsck 速度变慢。因此，请选择默认的 inode ratio，以确保索引和点的总数仍然低于这个限制。OST 文件系统检查时间受到包括索引和点数量在内等一系列变量的影响，如文件系统的大小、分配的块数量、分配块在磁盘上的分布、磁玛速度、CPU GREE. AR ae EA内存数量。对于正靖运行的文件系统，合理的文件系统检查时间大概在每 TiB 5-30 分钟左右，但如果检测到大量错误并需要修正，时间则会显若增加。53\nLustre 文件系统操作手册译者:这ay5.4. 文件和文件系统的极限值下表描述了当前已知 Lustre 相关了最大指标值。这些值受限于 Lustre 体系结构、Linux虚拟文件系统 (VFS) 或虚拟内存子系统。其中少数值是在代码中定义的，通过重新编译Lustre 软件可以进行更改。可利用以下例子中这些极限值测试 Lustre 软件。名称最大 MDTs数量最大 OSTs数量最大 OST大小最大客户器数量最大单个文件系统大小最大条人带数值2308150512TiB(Idiskfs),512TiB (ZFS)131072至少 1EiB2000描述一个MDS 可以承载多个MDT，每个MDT 可以是一个单独的文件系统。最多可以将 255 个MDTs 添加到文件系统，并使用 DNE 远程或条带目录将其附加到名称空间中。OST 的最大数量是一个可以在编译时改变的浓量。Lustre 文件系统已经测试了多达 4000 个 OSTs.ZB OST 文件系统可以配置在单个 OSS Fi AE.这不是一个硬性限制。也可以配置更大的 OST，但是大多数生产系统通常不会超过该限制，为 Lustre 可以通过增加视外的 OSTs 来提升容量和人性能以及I/0 总体性能，尽量减少竞争并多许并行恢复 〈e2fsck Bk scrub) .对于 32 位内核，由于页面缓存限制，",
        "可以通过增加视外的 OSTs 来提升容量和人性能以及I/0 总体性能，尽量减少竞争并多许并行恢复 〈e2fsck Bk scrub) .对于 32 位内核，由于页面缓存限制，最大块设备大小为 16TB ，这个大小也适用于 OST。强烈建议使用 64 位内核运行 Lustre 客户端和服务需。客户端的最大数量是一个可以在编译时改变的种量。在生产环境中使用了高达 30000 个客户端。每个 OST 可将其文件系统配置成最大 OST 大小，并且可将所允许的最大数量的 OSTs 组合成单个文件系统。该值受存储在磁盘上并以RPC 请求形式发送的布局信息大小限制，但这不是协议中的硬性限制。文件系统中的 OST 数量可以超过条带数量，单个54\nLustre 文件系统操作手册这ay名称 值最大条市大 <4GiB小By/)SitrK 64 KiB小最大单个对“16TiB象大小 (Idiskfs),256TiB (ZFS)最大文件大 16TiB (32小 位系统) 31.25PiB(64 位Idiskfs 系统)，8EiB (64 位ZFS 系统)单个目录下 1000 万个文件最大文件或 (Idiskfs), 2°48子目录效量 个文件 (ZFS)描述文件条带化的 OST 数量将受限于此。在移动到下一个对象前写入到每个对象的数据量。由于在某些 64 位机器 (如 ARM 和POWER) 上的 64 KiBPAGE SIZE 限制，最小条市大小被设置为 64KiB。这样单个页面就不会被拆分到多个服务硕上即可以存储在单个对象中的数据量。一个对象对应一个条带。ldiskfs 的限制为 16 TB, we AA TA个对象。对于 ZFS，该限制来目于底层 OST 的大小。文件最多可以包含 2000 个条带，每个条带可达到的最大对象大小。SARA EF KBR, FE 32 位系统上的单个文件大小最大为 16 TiB。在 64 位系统上，这个限制不存在。因此，如采后备文件系统可以文持足够大的对象或者文件很稀蕊，则文件大小可以是2 * 63位",
        "用户登录后会出现如图 3-1 的磁盘配额信息:Filesystem used quota Limit grace files quota kimt = grace/fs2 3616 045126 1T - 13942 1999966 2000000图 3-1 磁盘配额登陆提示信息22\nPr TH-eX 系统用户手册表 3-2 磁盘配额各关键词说明5 ee >| Rhesystem |用户所在的共享分布式存储it | rEpiles |用疡已有的文伯数量 (单位: 个)it | 文件数量硬限制 〈单位: 个)以磁盘存储为例说明软、硬限制的含义，文件数软、硬限制的含义与其一样。用户使用存储低于 512G 时，如图 3-1 所示，存储状态正常，当用户使用存储介于512G 和 1T 之间时，存储状态如图 3-2 所示，kbytes 参数对应的数字带有“*”表示用户配额异营，“6d23h59m57Ss”表示一个月的倒计时，如果用户在倒计时结束前将使用存储清理到 512G 以下，则存储状态恢复正常。和否则用户的数据量超出软限制且超出倒计时，如图 3-3 所示。如果用户数据在倒计时期间继续增长，超出硬限制，则用户存储将无法写入，如图 3-4 Stax; 数据操作也会受限制，如图 3-5 所人小。Filesystem used quota Limit grace files quota Ltntt grace/Ts2 728G* 5126 iT 6d23h59m57s =Filesystem used quota Limit grace files quota Limtt grace/Ts2 728G* 5126 iT 438942 =Filesystem used quota limit files quota limit = grace/fs2 1.237 ;23\nNSC[nscctj@th-ex-1n0] $ cp test.txt test2.txtcp:writing './test2.txt':Disk quota exceededcp:writing './test2.txt':Input/Output3-5 用户数据超出磁盘配额限制的状态提示VY. wt(EIS:有的时候用户登录会出现错误提示“Some errors happened when getting quotainfo. Some devices may be not working or deactivated.",
        "【已解决】不同系统du命令查询结果不同\n**标签**: du,  文件大小\n**创建时间**: 2022-01-28 16:03:15\n**更新时间**: 2022-02-25 09:15:26\n**作者**: 司道军\n**问题**：从HPC系统迁移数据到3F，拷贝完成后统计数据大小不同\n解决方法： 使用`du -b`命令检查，`du -b`命令等价于`du apparent-size block-size=1`，即以byte为单位统计文件的实际大小；。\n以下为`man du`手册中`apparent-size`选项的解释，即为文件的实际大小。\napparent-size\nprint  apparent  sizes,  rather  than  disk  usage; although the apparent size is usually smaller, it may be larger due to holes in ('sparse') files, internal fragmentation, indirect blocks, and the like\n使用du默认统计的磁盘占用空间大小，不同系统由于数据存储块等设置不同，可能统计的结果不同。因此可以通过du -b命令统计不同系统的结果是否一致，进行验证。\n参考https://stackoverflow.com/questions/37341281/disk-usage-du-showing-different-results"
    ]
}