{
    "query": "如何对thcp4队列的128GB节点全部执行加压操作？",
    "summaries": [
        "本文档描述了大屏机房运维总览的剧本执行过程，包括节点操作、存储分区操作、资源操作、服务操作、数据拷贝、应急操作和批量操作等。重点涉及对TH-HPC4集群中存储节点（如THL6、ost12、ost141等）的状态监控与维护，包括重启、关机、挂起、恢复作业等操作。同时，针对存储节点负载过高的问题，提供了查看CPU、内存使用情况及故障查询的方法，并通过统一监控运维平台进行审计和管理。文档还包含具体的操作步骤和状态信息，用于确保系统稳定运行。",
        "TH-HPC系统常见问题包括作业断开、内存不足、动态库缺失、作业被自动退出等。解决方法包括剔除问题结点、同步时间、调整资源申请、设置环境变量、使用yhbatch提交作业等。作业处于PD状态是因调度策略，需耐心等待。作业状态“S”表示被挂起，“CG”和“comp”需管理员处理。计算慢可能与存储、网络、残留进程或节点错误有关。命令缺失可复制登录结点命令并设置环境变量。权限问题需检查队列和资源限制。$SLURM_NPROCS对应PBS的$PBS_NODELINE。MPI运行错误可能由网络或节点问题引起，需联系管理员。",
        "该文本描述了在服务器 ln32 上使用 p4vasp 的步骤，包括通过 SSH 连接、加载 singularity 模块、执行镜像文件，并启动 p4v 程序。用户通过命令行操作，可进行结构、电子、力学等计算，支持 DOS 和 bands 分析、STM 图像生成等功能。操作过程中涉及的文件如 vasprun.xml 用于存储计算结果。"
    ],
    "contents": [
        "【已解决】3f-ln32 p4vasp\n**标签**: 无标签\n**创建时间**: 2024-11-21 11:18:05\n**更新时间**: 2024-11-21 11:18:05\n**作者**: 梁言\nssh -X ln32\nmodule load singularity\nsingularity exec /thfs1/home/chengroup/software/p4vasp-ubuntu16.simg /app/p4vasp/bin/p4v\n#镜像也可在其他分区使用\np4v.py@In32\nFile Edit Structure Electronic Convergence Mechanics Database\nNew\n=)\n/ System: ??? (vasprun.xml)\n|Selection:|\nInfo\nOpen\na\nShow\na\nBy\nControl\n£\nBuild\nDOS+bands\nwi\nSTM\nCommit\nDescription:\nOK",
        "大屏机房运维总览剧本执行\n\n时\n其人操作 节点操作.一输入节点名称\n\nCoa 选择重启/开机/关机\n\nTH-HPC4\n\n器 ce TH-HPC\n中 存储分区操作\n中 资源操作\n\n剧本执行加 用户操作Le]\n\n2.ee)iF\n\n“中 服务操作\n\n忠孝所拷贝\n\nCo 应忽操作\n\n口 批量操作\n\n已其也操作\n4）查看分区链接数，确认ost的链接数已经恢复。\n正常状态：链接数与其他ost一致，并且是running（healthy）状态。\nTH-HPC\n节点操作\n\n TH-HPCA© TH-HPC > THL6\n\n8 ofa]y\n\n日 © 存储分区操作\n\n加 THL5\n\n分区作业恢复分区作业挂起\n\n剧本执行\n\n加THL7\n\nca?THs\n\nTHL6查询链接数 X\n\n局 用户操作© ok: [121.16.225.1] => {正常的链接数状态 vi\n\n© 作业操作\n: THL6-MDTeeee: 561 ， running(healthy)加\n口 服务操作:::-\n: THL6-0sTeeee: 497 ”running(healthy)THL6-0sTeee1: 497 ”running(healthy)\nO 数据拷贝: THL6-OST@@02: 497 running(healthy)THL6-0sT6663: 497 ”running(healthy)\n号 应急操作: THL6-OST@0@4: 497 ”running(healthy)THL6-0sTeee5: 497 running(healthy)\n口 批量操作: THL6-0sT6666: 497 ”running(healthy)THL6-0sT6687: 497 ”running(healthy)\n-\"ost12: THL6-OST0008: 497 ”running(healthy)THL6-0ST@@09: 497 running(healthy)\n吕 其他操作\"ost13: THL6-0ST896a: 497 ”running(healthy)THL6-0sTeeeb: 497 ”running(healthy)\nTH-eX\"ost14: THL6-0SsT86ec: 497 ”running(healthy)THL6-OSTeeed: 497 running(healthy)\nTH-3F\"ost15:",
        "的共享存储。用户提交可以加-x剔除问题结点，然后联系管理员进行解决。\nQ：作业断开，slurm日志中出现“yhrun: error: Task launch for 2440965.0 failed on node cn2892: Job credential expired”报错信息\nA：这是由于计算结点时间没有与管理结点同步。用户提交可以加-x剔除问题结点，然后联系管理员进行解决。\nQ：作业断开，slurm日志中出现“bus error”报错信息\nA：导致“bus error”的报错原因很多，具体问题需要使用工具排查。用户提交可以加-x剔除问题结点，然后联系管理员进行解决。\nQ：运行作业报错“forrtl: severe (41): insufficient virtual memory\"\nA：运行作业的内存不足，请尝试多使用结点，每个结点上少使用核数来提交运行。\nQ：运行作业提示“error while loading shared libraries: libXXX.so: cannot open shared object file: No such file or directory”\nA：需要用户将动态链接库的路径添加到自己运行的环境变量中，假设缺少x库，先“locate x”找到该链接库的地址$DIR，请确保$DIR为共享目录！然后编辑用户目录下的配置文件~/.bashrc，添加“export LD_LIBRARY_PATH=$DIR:$LD_LIBRARY_PATH”。\n在计算时找不到动态库是因为计算结点和登陆结点的软件环境有所不同。链接器在处理动态库时将链接时路径（Link-time path）和运行时路径（Run-time path）分开，-L只是指定了程序链接时库的路径，并不影响程序执行时库的路径；-Wl,-rpath指定程序运行时库的路径，该库的路径信息保存在可执行文件中，运行时它会直接到该路径查找库；也可使用LD_LIBRARY_PATH环境变量来指定动态库在运行时的搜索路径。\nQ：提交的作业总是被自动退出\nA：用yhrun提交任务不是非常稳定，比如终端关闭，脚本终止会导致任务被杀掉。建议用户使用yhbatch的提交方式，yhbatch提交的任务，终端关闭不会有任何影响，登陆节点down机也不会有影响。\nyhbatch的提交方法和",
        "系统存储和网络正常，然后检查用户作业是否有其他用户残留进程，有的话杀掉。最后检查节点是否有报clocksource错，有的话将节点drain掉，告知用户再提交时-x剔除问题节点。\nQ：在计算结点上运行程序，找不到某些命令，比如说提示 bc: Command not found\nA：复制登录结点上的bc命令到自己账户下，设置好该命令的环境变量后，重新运行就可以找到命令。\nQ：提交作业后，提示 “yhbatch: error: Batch job submission failed: User's group not permitted to use this partition”和“Batch job submission failed : Job violates accounting/QOS policy(job submit limit, user's size and/or timelimits”\nA：用户没有权限使用提交作业时-p参数后面指定的队列，请使用yhi命令检查您可以使用的队列。后者是因为提交作业所需要的资源使用权限超过了当前用户所拥有的资源使用权限。\nQ：PBS作业系统里查看运行的结点名称的变量 $PBS_NODELINE，在TH-HPC里对应哪一个变量\nA：$SLURM_NPROCS，它与PBS的$PBS_NODELINE是一样的功能。\nQ：使用天河software目录下的一个mpi实现编译程序，运行时slurm文件中提示报错：\nGLEX_ERR(cn1368): _Progress(172), err CQE:status=Dest_Key:opcode=RDMA_WRITE:signaled=1:rmt_nic_id=1370\nyhrun: Job step aborted: Waiting up to 2 seconds for job step to finish.\nFatal error in PMPI_Bcast: Other MPI error, error stack:\nMPIDI_CH3I_Progress(176): progress engine failure\nIn: PMI_Abort(1, Fatal error in PMPI_Bcast: Other MPI error, error stack:\nMPIDI_CH3I_Progress(176): progress engine failure)\nslurmd[cn1368]: *** STEP 2796179.0 KILLED AT 2015-10-12T11:27:12 WITH",
        "THL6-0sTeeeb: 497 ”running(healthy)\nTH-eX\"ost14: THL6-0SsT86ec: 497 ”running(healthy)THL6-OSTeeed: 497 running(healthy)\nTH-3F\"ost15: THL6-OSTe@ee: 497 ”running(healthy)THL6-0sTeeef: 497 running(healthy)\n\"ost16: THL6-0ST010: 497 ”running(healthy)THL6-osTeel1: 497 ”running(healthy)\n\nTH-3M\n\n\"ost17: THL6-0ST6912: _497iTHL6-OST@Q13: _497\n如果重启的ost链接数少1或者少2，需要查询登陆节点挂载情况。\n5）恢复作业\n统一监控运维平台= 运维管理\n\n定制大屏剧本执行\n\n节点操作\n\nTH-HPC4\n日 © 存储分区操作\n加 THL5\n加THL7\n加 THL8\n\n执行审计\n\nTH-HPC\n\n全 TH-HPc > THL6\n\nAr\n\n分区作业挂起\n3.3.4 ost负载过高\n设备名\n\nost141\n\n负载过高\n\n集群\n\nTH-HPC4\n\n存储节点\n\n类型\n\n硬件\n\n严重程度\n\ne 警告\n\n=o\n查看ost的cpu和内存的使用情况，参考下图。\n统一监控运维平台\n\n其他操作 节点操作\n\nost141\n\n日 GTH-HPC4\n日 4-3\n日 storage\n\nRNaDosti41\n\nTH-HPC4\n\nec 节点编号: ost141\n序号: 1216\n节点名称: ost141\n\n节点类型: 存储节点\n\n查询raid卡日志-…\n\n所属集群 TH-HPC4硬盘大小- 无硬盘\n\n所属分区: _null硬盘类型. 无硬盘\n\n存储位置: 新机房3-5-TH-HPC4-4-3-23.0节点状态: co ]\n\nARSARC\n\ncpu进程排序mem进程排序\n还能够根据“故障查询”查询导致负载高的作业情况。\n统一监控运维平台\n\n定制大屏剧本执行运维总览\n\n集群TH-3KTH-3MTH-3FTH-eXTH-HPC TH-HPC4\n\n来源gluster节点gpu节点ION节点 存储节点接口设备登录节点管理节点网络设备计算",
        "非常稳定，比如终端关闭，脚本终止会导致任务被杀掉。建议用户使用yhbatch的提交方式，yhbatch提交的任务，终端关闭不会有任何影响，登陆节点down机也不会有影响。\nyhbatch的提交方法和步骤如下：\n1）准备一个 bash 脚本（csh脚本也行），格式和run.sh类似，只是不需要再进行输出的重定向了。\n2）yhbatch提交那个脚本，提交方式为yhbatch -N XXX-n ZZZ-p YYY ./sub.sh 类似。\n假设用户可执行文件为part，则sub.sh脚本可以这样写：\n#! /bin/bash\nyhrun -n 36 -p TH_NET /vol-th/home/username/part\n则yhbatch提交任务如下：\nyhbatch -N 3 -p TH_NET ./sub.sh\n或者yhbatch -n 36 -p TH_NET ./sub.sh\n只要保证yhbatch申请的资源不小于yhrun需求的资源即可。\n另外，用户可以根据作业调度系统日志来判断退出原因，是否与以上问题类似。\n注意：存储ost掉链接、重启都有可能导致用户掉作业。\nQ：查看有可用结点，但作业却一直处于PD状态\nA：TH-HPC系统的资源管理器采用“先进先出”的作业调度方式，作业处于PD状态说明在用户前面有其他用户先提交了作业，并且之前的用户作业超出了目前的可用资源总数，请用户耐心等待。根据用户资源需求，系统管理人员也会定期进行资源调整，降低作业排队时间。\nQ：作业状态“S；CG；comp“分别是什么原因？\nA：“S”表示管理员将用户作业挂起以进行故障检测或故障处理，处理完后会将该作业恢复，不会对作业产生任何影响；“CG”是由于该作业没有正常推出导致，需管理员重启节点；“comp”是作业异常导致，需管理员关闭节点。\nQ：作业为什么计算慢？\nA：先确定系统存储和网络正常，然后检查用户作业是否有其他用户残留进程，有的话杀掉。最后检查节点是否有报clocksource错，有的话将节点drain掉，告知用户再提交时-x剔除问题节点。\nQ：在",
        "统一监控运维平台\n\n定制大屏剧本执行运维总览\n\n集群TH-3KTH-3MTH-3FTH-eXTH-HPC TH-HPC4\n\n来源gluster节点gpu节点ION节点 存储节点接口设备登录节点管理节点网络设备计算节点其他\n类型硬件安全服务环境\n\n严重程度通知警告严重灾难\n\n是否修复未处理处理\n\n+ 起止日期2024-06-17 16:57:352024-06-24 16:57:35\n\nfae\n\n描述集群来源类型严重程度状态\n负载过高TH-HPC4存储节点硬件。 警告已处理\nost127负载过高TH-HPC4存储节点硬件。 警告已处理\n统一监控运维平台\n\n定制大屏剧本执行运维总览ia\n\n节点名称: ost127\nFRAME): 2024-06-19T16:58:13故障类型: HARDWARE故障描述: 负载过高\n\n>节点资源使用情况图形展示\n\n88 存储节点作业模板\n\nhosthostjobid值\n\nost1271818914ost12718232582184\nost 12718277724851ost 12718189141143\nost 12718278553418ost 2718274027.89\nost 127182787524.09ost 27sftp-server.20654373\nost 127182785823.06ost 127node.20912245\nost 127182787220.54ost 271768786137\nost 12718278712047ost 27bash204611.19\nost 12718274022.39ost 127sftp-server.20528,O71\nost 127182509916ost 12717968960.69\n\nost12718257344.21ost127182582803\n88 存储节点作业模板\n\nost127\nost127\nost127\nost127\nost127\nost127\nost127\nost127\nost127\nost127\n\nED\n\nost127\nost127\nost127\nost127\nost127\nost127\nost127\nost127\n\nSas\njobid\n1818914\n1827772\n1827855\n1827875\n1827858\n1827872\n1827871\n1827402\n1825099\n1825734\n\n写次数\n\n‘bash 20461\n1823258\n1807724\n1818914\n1796896\n1827402\n\nMATLAB.20634\n\nnode.20461\nconda.20246\n\n值\n257.01\n48.51\n34.18\n24,09\n23.06\n20.54\n20.87\n2.39\n16\n121\n\na4\n5.46\n3.02\n2.87\n2.48\n197\n197\n1.07\n1.02\n\nost27\nost127\nost127\nost127\nost127\nost127\nost127\nost127\nost127\n\nost127\nost127\nost127\nost127\nost127\nost127\nost127\nost127\nost127\n\n—\n\njobid\n\n1828258\n1818914\n1827402\n\nsftp-server.20654\n\nnode.20912\n1768786\nbash20461\nsftp-server.20528,\n1796896\n1825828\n\n读次数\n\njobid\n\n1818914\n1827772"
    ]
}