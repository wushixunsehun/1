{
    "query": "HPC3系统的bowen账号运行任务报错，如何检查是否是内存溢出？给出具体步骤。",
    "summaries": [
        "HPC4 GPU运行deepmd-kit时出现“RESOURCE_EXHAUSTED: OOM”错误，原因是计算体系过大，导致显存不足。报错信息显示在分配形状为[1,988542000]的双精度张量时发生内存溢出。通过增加显卡数量以扩充显存总数，问题得以解决，计算恢复正常。",
        "用户询问如何查看计算节点的内存使用情况。首先通过命令yhq查找任务所使用的节点，确认节点为cn21。然后登录到该节点，使用top或free -g命令查看内存使用情况。此问题已解决。",
        "HPC2系统使用MPI/openmpi-4.0.0/intel2018u4时，通过yhrun运行程序报错。问题可能源于OpenMPI 4.0后默认配置变化，导致直接编译运行失败。错误信息显示与InfiniBand设备初始化相关，建议设置`mca btl ^openib`。提交任务时报错涉及PMI支持缺失，需配置SLURM的PMI或PMIx支持。此外，UCX相关错误提示缺少ib_ucm.ko模块。总结：需调整OpenMPI配置并确保SLURM和UCX依赖正确安装。"
    ],
    "contents": [
        "【已解决】HPC2系统 MPI/openmpi-4.0.0/intel2018u4 使用 yhrun 报错\n**标签**: mpi,  openmpi,  yhruin\n**创建时间**: 2021-09-29 18:00:08\n**更新时间**: 2021-10-15 15:56:43\n**作者**: 郑刚\n**问题**：HPC2系统 MPI/openmpi-4.0.0/intel2018u4 使用 yhrun 报错\n可能由于 openmpi-4.0.0 之后，默认配置发生了改变，因此直接编译后使用存在问题，建议为：\nmca btl ^openib\n报错记录\n直接加载、编译、运行，报错如下：\n[zhenggang2@th-hpc2-ln0 mpi]$ module purge\n[zhenggang2@th-hpc2-ln0 mpi]$ module add Intel_compiler/18.0.4\n[zhenggang2@th-hpc2-ln0 mpi]$ module add MPI/openmpi-4.0.0/intel2018u4\n[zhenggang2@th-hpc2-ln0 mpi]$ mpicc mpihello.c\n[zhenggang2@th-hpc2-ln0 mpi]$ ./a.out\nBy default, for Open MPI 4.0 and later, infiniband ports on a device\nare not used by default.  The intent is to use UCX for these devices.\nYou can override this policy by setting the btl_openib_allow_ib MCA parameter\nto true.\nLocal host:              th-hpc2-ln0\nLocal adapter:           mlx5_0\nLocal port:              1\nWARNING: There was an error initializing an OpenFabrics device.\nLocal host:   th-hpc2-ln0\nLocal device: mlx5_0\nHelloWorld!Process      0       of      1",
        "th-hpc2-ln0\nLocal device: mlx5_0\nHelloWorld!Process      0       of      1       on      th-hpc2-ln0\n尝试提交任务报错如下：\n[zhenggang2@th-hpc2-ln0 mpi]$ yhrun -N 1 -n 1 -p debug2 ./a.out\n[cn553:29526] OPAL ERROR: Not initialized in file pmix3x_client.c at line 113\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM's PMI support and therefore cannot\nexecute. There are several options for building PMI support under\nSLURM, depending upon the SLURM version you are using:\nversion 16.05 or later: you can use SLURM's PMIx support. This\nrequires that you configure and build SLURM with-pmix.\nVersions earlier than 16.05: you must use either SLURM's PMI-1 or\nPMI-2 support. SLURM builds PMI-1 by default, or you can manually\ninstall PMI-2. You must then build Open MPI using with-pmi pointing\nto the SLURM PMI library location.\nPlease configure as appropriate and try again.\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[cn553:29526] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not",
        ":GPU:0 by allocator GPU_0_bfc\n[[{node gradients/Slice_7_grad/Pad}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom\nto RunOptions for current allocation info. This isn't available when running in Eager mode.\n6 successful operations.\n9 derived errors ignored. (/home/conda/feedstock_root/build_artifacts/libdeepmd_1663923207577/work/source/lmp/\npair_deepmd.cpp:390)\nLast command: run             50000\n**2.报错原因及解决**\n计算体系较大（具体体现为所计算的原子数较大），导致显存不足，通过增加显卡数量，扩充显存总数后，得以正常计算",
        "【已解决】HPC4 GPU运行deepmd-kit报DeePMD-kit Error: TensorFlow Error: RESOURCE_EXHAUSTED: 2 root error(s) found.\n**标签**: 无标签\n**创建时间**: 2023-10-19 14:58:42\n**更新时间**: 2023-10-19 14:58:42\n**作者**: 杜思慧\n**1.具体报错如下**\n6 successful operations.\n6 derived errors ignored.\nERROR: DeePMD-kit Error: TensorFlow Error: RESOURCE_EXHAUSTED: 2 root error(s) found.\n(@) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,988542000] and type double on /job: localhost\n/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n[[{node gradients/Slice_7_grad/Pad}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom\nto RunOptions for current allocation info. This isn't available when running in Eager mode.\n[[o_force/_31]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom\nto RunOptions for current allocation info. This isn't available when running in Eager mode.\n(1) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1,988542000] and type double on /job: localhost\n/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n[[{node gradients/Slice_7_grad/Pad}]]\nHint: If you want to see a list",
        "【已解决】用户询问如何查看计算节点的内存使用情况\n**标签**: 无标签\n**创建时间**: 2021-11-12 17:30:53\n**更新时间**: 2021-12-10 15:43:11\n**作者**: 杜思慧\n**1.yhq查看任务所使用的节点**\n查到所用节点包含cn21\n**2.登录到计算节点“ssh cn21”**\n**3.两种查看方式：**\n**（1）top**\n**(2) free -g**",
        "MPI job)\n[cn553:29526] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\nyhrun: error: cn553: task 0: Exited with exit code 1\n编写脚本提交报错如下：\n1 libibcm: couldn't read ABI version\n2 [1633677290.690764] [cn553:29838:0]       cm_iface.c:290  UCX  ERROR ib_cm_open_device() failed: No such file or directory. Check if ib_ucm.ko module is loaded.\n3 libibcm: couldn't read ABI version\n4 [1633677290.691263] [cn553:29839:0]       cm_iface.c:290  UCX  ERROR ib_cm_open_device() failed: No such file or directory. Check if ib_ucm.ko module is loaded.\n5 libibcm: couldn't read ABI version\n6 [1633677290.693403] [cn554:2682 :0]       cm_iface.c:290  UCX  ERROR ib_cm_open_device() failed: No such file or directory. Check if ib_ucm.ko module is loaded.\n7 libibcm: couldn't read ABI version\n8 [1633677290.695769] [cn554:2681 :0]       cm_iface.c:290  UCX  ERROR ib_cm_open_device() failed: No such file or directory. Check if ib_ucm.ko module is loaded.\n9 [cn553:29838] pml_ucx.c:212 Error: Failed to create UCP worker\n10 [cn553:29839] pml"
    ]
}