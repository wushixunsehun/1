{
    "query": "计算3F 集群的 CPU 整体利用率是多少？",
    "summaries": [
        "本文介绍了通过 `yhrun jobid=<job_id> nvidia-smi` 命令查询 GPU 利用率的方法，适用于 k80 集群。测试显示，VASP 可成功查询 GPU 使用情况，而 LAMMPS、Python、GROMACS 等软件无法查询，可能与作业调度系统有关。同时，查询过程中出现“Requested nodes are busy”提示，表明节点可能处于忙碌状态。",
        "该文本记录了GPU使用情况的监控数据，显示GPU 0占用率高达98%，使用了1542MiB显存，而其他GPU（1、2、3）使用率均为0%，仅消耗3MiB显存。同时提到用户程序仅使用了GPU的25%计算资源，存在资源浪费问题，建议进行计算调整。用户通过命令`yhbatch -N 1 -n 1 -p TH_GPU ./sub.sh`提交任务，并通过`nvidia-smi`查看GPU状态。",
        "该文本展示了GPU使用情况，显示GPU 0占用约98%的计算资源，而其他GPU未被使用。程序仅使用了GPU的25%计算资源，存在资源浪费。建议用户调整计算设置以提高利用率。提交脚本为`yhbatch -N 1 -n 1 -p TH_GPU ./sub.sh`，并可通过`nvidia-smi`查看GPU状态。"
    ],
    "contents": [
        "149W |   1542MiB / 11441MiB |     98%      Default |\n|                               |                      |                  N/A |\n++++\n|   1  Tesla K80           Off  | 00000000:85:00.0 Off |                    0 |\n| N/A   23C    P8    30W / 149W |      3MiB / 11441MiB |      0%      Default |\n|                               |                      |                  N/A |\n++++\n|   2  Tesla K80           Off  | 00000000:8B:00.0 Off |                    0 |\n| N/A   22C    P8    26W / 149W |      3MiB / 11441MiB |      0%      Default |\n|                               |                      |                  N/A |",
        "|\n||\n|    0   N/A  N/A     29423      C   ...conda_2020.07/bin/python3     1539MiB |\n++\n```\n4. 问题\n用户该程序只能使用GPU的25%计算资源，有些浪费，联系用户进行计算调整\n#!/bin/bash\nyhrun -N 1 -n 1 -p TH_GPU python3 /THL5/home/gtcao/ljw/MedMNIST/train.py\n2. 提交\n```bash\nyhbatch -N 1 -n 1 -p TH_GPU ./sub.sh\n```\n3. 查看GPU使用情况\n```bash\n[gtcao@gn2 ~]$ nvidia-smi\nThu Sep 30 09:53:27 2021\n++\n| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n|+++\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|++|\n|   0  Tesla K80           Off  | 00000000:84:00.0 Off |                    0 |\n| N/A   56C    P0   144W /",
        "【测试中】利用yhrun查询gpu利用率\n**标签**: 无标签\n**创建时间**: 2023-11-16 11:13:20\n**更新时间**: 2023-11-17 11:13:39\n**作者**: 杜思慧\n**1. 查询语句**\n#该方法也适用于k80集群\nyhrun jobid=<job_id> nvidia-smi\n2.测试情况\n单卡查询：\n目前仅vasp可同通过该方法查询，其他软件无法查询疑似和作业调度系统有关\nvasp\n[dush2Gth-hpc4-Lng ~]$ yhq\nJOBID PARTITION     NAME     USER ST       TIME NODES NODELIST(REASON)\n1443650       gpu   sub.sh    dush2 R       2:06      1 gn36\n[dush2@th-hpc4-1tn0 ~]$ yhrun jobid=1443650 nvidia-smi\nThu Nov 16 11:12:51 2023\n+十\n| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5\n|  2-2 rere rere rere re eee ee++十\n| GPU Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC\n| Fan Temp Perf Pwr:Usage/Cap|         Memory-Usage | GPU-Util Compute M.\n|                        |                MIG M. |\n一一=一一一一一一一一一一=一一一一一一一一一一一一一一一一一二一一一一一一一一一一一一一一=一一=一一=一+一|\n|   9 NVIDIA A100 80G... Off | 00000000:4B:00.0 Off",
        "Usage      |\n||\n|    0   N/A  N/A     29423      C   ...conda_2020.07/bin/python3     1539MiB |\n++\n```\n4. 问题\n用户该程序只能使用GPU的25%计算资源，有些浪费，联系用户进行计算调整\nyhbatch -N 1 -n 1 -p TH_GPU ./sub.sh\n3. 查看GPU使用情况\n```bash\n[gtcao@gn2 ~]$ nvidia-smi\nThu Sep 30 09:53:27 2021\n++\n| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n|+++\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|++|\n|   0  Tesla K80           Off  | 00000000:84:00.0 Off |                    0 |\n| N/A   56C    P0   144W / 149W |",
        "|                      |                  N/A |\n++++\n|   3  Tesla K80           Off  | 00000000:8C:00.0 Off |                    0 |\n| N/A   34C    P8    30W / 149W |      3MiB / 11441MiB |      0%      Default |\n|                               |                      |                  N/A |\n++++\n++\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n||\n|    0   N/A  N/A     29423      C   ...conda_2020.07/bin/python3",
        ":00.0 Off |                    0 |\n| N/A   56C    P0   144W / 149W |   1542MiB / 11441MiB |     98%      Default |\n|                               |                      |                  N/A |\n++++\n|   1  Tesla K80           Off  | 00000000:85:00.0 Off |                    0 |\n| N/A   23C    P8    30W / 149W |      3MiB / 11441MiB |      0%      Default |\n|                               |                      |                  N/A |\n++++\n|   2  Tesla K80           Off  | 00000000:8B:00.0 Off |                    0 |\n| N/A   22C    P8    26W / 149W |      3MiB / 11441MiB |      0%      Default |\n|",
        "/ 11441MiB |      0%      Default |\n|                               |                      |                  N/A |\n++++\n|   3  Tesla K80           Off  | 00000000:8C:00.0 Off |                    0 |\n| N/A   34C    P8    30W / 149W |      3MiB / 11441MiB |      0%      Default |\n|                               |                      |                  N/A |\n++++\n++\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage",
        "一一一一一一一一一一一一一一一一一二一一一一一一一一一一一一一一=一一=一一=一+一|\n|   9 NVIDIA A100 80G... Off | 00000000:4B:00.0 Off |                    0\n| NMA 53C    PQ 300W / 300W | 37811MiB / 81070MiB |    100%      Default\n|                                 |                        |              Disabled |\nf4+4++\n+十\n| Processes:\nGPU   GI   cI        PID Type   Process name                  GPU Memory\nID ID                                                   Usage\nlammps。python，gromacs等均无法查询\n[dush2@th-hpc4-1n1 accelerate]$ yhq\nJOBID PARTITION     NAME     USER ST       TIME NODES NODELIST(REASON)\n1443675       gpu   sub.sh    dush2 R       0:15      1 gn36\n[dush2@th-hpc4-1n1 accelerate]$ yhrun jobid=1443675 nvidia-smi\nlyhrun: Job 1443675 step creation temporarily disabled, retrying (Requested nodes are busy)"
    ]
}