{
    "query": "eX某账号大量作业卡住，如何检查原因？",
    "summaries": [
        "TH-3F系统运行calypso.x和vasp时出现“Requested nodes are busy”错误，导致作业无法提交。问题可能由节点资源不足或内存分配不当引起。解决方法包括：将vasp作业核数从64改为56以减少资源占用；在yhrun命令中添加mem=100GB限制内存使用；尝试使用mpi-n编译的vasp并用mpirun调用。此外，建议设置NPAR=4、KPAR=1以优化计算效率。",
        "系统在运行过程中出现错误，提示“ERROR failed to register user buffer datatype”，涉及地址和长度信息，可能与内存或I/O操作有关。随后出现多个UCX错误日志，均指向glex_md.c文件的362行，表明在注册用户缓冲区时发生问题。最后，任务被中止，显示“Aborted”和“STEP 3596459. ON cn1944 CANCELLED AT”，表明作业执行失败，可能与通信库或资源管理器相关。",
        "系统出现进程引擎故障，作业被信号9终止。MPI版本问题可能导致错误，建议替换.bashrc中的编译器和MPI路径。作业运行中可能因系统维护被挂起，需手动终止并续算。程序因编译与运行环境不一致导致AVX支持错误，应移除-xHOST/-xAVX选项。存储配额默认为500G软限制、1T硬限制，超限将无法写入。IO错误可能由存储压力或OST满载引起。ls命令卡顿可能因节点负载高、网络延迟或存储恢复。GPU无法识别可能因PCIe连接松动。"
    ],
    "contents": [
        "ERROR failed to register user buffer datatype @x8 address @x4e00ac497010 len 344964: Input/output error\n日\n1\n2\n3\n4\n5\n6\n7\n8\n9\n/th¥s1/software/mpich/mpi-x-gcc1@.2.0/1ib/Libmpi.so.12(PMPI_Recv+0x294) [ex488817815f44]\n/th¥s1/home/wf1iue6/dy /PanguLU-4.1.@/examples/./pangulu_example.elf(+@x16ed8) [@xaaaaeSa49ed8]\n/th¥s1/home/wf1iu6/dy /PanguLU-4.1.@/examples/./pangulu_example.elf(+@x1883@) [@xaaaaeSa4b830]\n18 /thfs1/home/wf1iu@6/dy/PangulU-4.1.@/examples/../pangulu_example.elf(+0x19078) [@xaaaaeSa4c078]\n311 /thfs1/home/wf1iue6/dy/PanguLU-4.1.0/examples/ ./pangulu_example.elf(+0x5334) [@xaaaaeSe38334]\n12 /ths1/home/wf1iue6/dy/PanguLU-4.1.0/examples/./pangulu_example.elf(+0x3@a8) [@xaaaaeSe360a8]\n343 /Lib/aarch64-Linux-gnu/libc.so.6(libc_start_main+@xe8) [0x4¢00172ed090]\n314 /thfs1/home/wf1iue6/dy/PanguLU-4.1.0/examples/./pangulu_example.elf(+0x34b4) [@xaaaaeSe364b4]\n[1727595377.588341] [cn1945:3200030:0]      glex_md.c:362 UCX ERROR GLEX cre\n[1727595377.588557] [cn1945:3260030:0]     glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588608] [cn1945:3200030:0]    glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588639] [cn1945:3200030:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588675] [cn1945:3200030:0]     glex_md.c:",
        "【已解决】TH-3F系统计算calypso.x & vasp (Requested nodes are busy)\n**标签**: calypso.x & vasp\n**创建时间**: 2022-11-08 15:42:14\n**更新时间**: 2022-11-08 15:42:14\n**作者**: 刘栋杰\n**问题**：(Requested nodes are busy)\nTH-3F系统计算calypso.x & vasp\n运行脚本\ncaly.sh\n#!/bin/bash\n#SBATCH  job-name=lixing\n#SBATCH  output=log.out.%j\n#SBATCH  error=log.err.%j\n#SBATCH  partition=thcp1\n#SBATCH  nodes=1\nexport UCX_TLS=sm,tcp\n# module load fftw/3.3.8-gcc4.9.3  # 环境里已加载，这行注释或删除\nmodule load python/2.7.18\n./calypso.x > caly.log 2>&1  # 此行进行修改\nsubmit.sh\n#!/bin/sh\nexport UCX_TLS=sm,tcp,glex\nEXE=vasp_std # choose one vasp version to run. e.g. vasp / vasp_ncl / vasp_gam / vasp_neb ...\nkillall -9 $EXE\nyhrun -p thcp1 -n  56 exclusive -w $SLURM_NODELIST mem=100GB $EXE > log 2>&1\n如果使用64核作业还是存在被杀的情况，建议使用56核进行计算，把脚本中64改成56即可。\n报错1\nyhrun: Job 1663451 step creation temporarily disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step",
        "retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\nyhrun: Job 1663451 step creation still disabled, retrying (Requested nodes are busy)\n测试方案1 无效\n尝试设置作业内存， `step creation temporarily disabled, retrying (Requested nodes are busy)`的原因是，首先执行的`yhrun`命令分配了所有内存。 为了解决这个问题，首先可选（？）在`yhbatch`中指定总内存分配：\n#SBATCH mem=120GB   #此参数暂时先不设置，不设置默认使用全部，物理内存128G，去除其他内存开销，限制124G可正常提交作业。\nvasp脚本\nyhrun 增加 mem=100GB # vasp使用内存限制在100GB，可根据需求调整\n测试方案2 无效\nkill vasp 进程后进行等待\n#!/bin/sh\nexport UCX_TLS=sm,tcp,glex\nEXE=vasp_std # choose one vasp version to run. e.g. vasp / vasp_ncl / vasp_gam / vasp_neb ...\nkillall -9 $EXE\nsleep 1s\nyhrun -p thcp1 -n  56 exclusive -w $SLURM_NODELIST mem=100GB $EXE >",
        "stack:\nMPIDI_CH3I_Progress(176): progress engine failure)\nslurmd[cn1368]: *** STEP 2796179.0 KILLED AT 2015-10-12T11:27:12 WITH SIGNAL 9 ***\nslurmd[cn1368]: *** STEP 2796179.0 KILLED AT 2015-10-12T11:27:12 WITH SIGNAL 9 ***\nA：该错误提示一般是由mpi版本导致。解决方法：使用/vol6/source.sh中的内容替换原~/.bashrc中关于intel编译器、mpi的路径。\nQ:任务提交运行后，有时在还未达到队列的时间天数期限时，运行的程序已“停止工作”（输出文件没有更新），但是通过作业查询命令（yhq）查看，作业看起还在R运行。\nA:遇到这个情况，请您及时手动杀掉您的作业，从断掉的地方接着续算就可以了。\nQ:输出的slurm文件中是如下数据：yhrun: got SIGCONT。我在天河服务器用户手册上没找到这条数据的解释。请问这条数据代表什么意思?\nA:这个是系统管理员临时维护系统，为了避免影响用户的作业，而把用户的作业挂起了出现的提示了。\nQ程序运行报错：Fatal Error: This program was not built to run in your system. Please verify that both the operating system and the processor support Intel(R) AVX. yhrun: error: cn2375: task 0: Exited with exit code 1\nA：该错误说明程序的编译时环境和运行时环境不一致，即程序编译时使用了支持AVX的选项，运行时的硬件环境不支持该AVX优化。\n一般这种情况发生是由于用户在编译程序时加入-xHOST/-xAVX选项（或是在安装软件时，系统自动读取到登陆节点上CPU的flag支持avx，故在编译软件时加入了-xHOST），那程序就会根据登陆节点的CPU配置信息进行优化编译，然而程序的运行是在计算节点上，计算节点的CPU配置信息可能不支持AVX，就与登陆节点不同，就会报",
        "vasp_neb ...\nkillall -9 $EXE\nsleep 1s\nyhrun -p thcp1 -n  56 exclusive -w $SLURM_NODELIST mem=100GB $EXE > log 2>&1\n无效\n测试方案3\nmpi-n编译vasp，使用mpirun调用，可正常运行，计算速度略慢。\n#!/bin/sh\n#SBATCH exclusive\n#SBATCH -w $SLURM_NODELIST\n#SBATCH mem=80GB\nexe=/thfs1/home/yanggc/5.4.4-opblas-gcc9.3.0-mpi-x/mpi-n/vasp_std\nexport UCX_TLS=sm,tcp\nkillall -9 vasp_std\nsleep 1s\nmpirun -np 64  $exe > log 2>&1\nVASP参数设置\n建议设置:   其中单节点测试中，32~56核，以下参数最优。\nNPAR = 4\nKPAR = 1",
        "“lfs quota –g username /vol-th”查看账号配额会看到已使用存储的数字旁边有一个“*”号，状态“4w1d23h59m57s”表示一个月的倒计时，如果用户在倒计时结束前将使用存储清理到500G以下，则存储状态恢复正常，否则，用户存储无法写入；如果用户使用存储大于1T，用户会无法写入。\nQ：磁盘无法写入，报“quota error”错误\nA：这是由于用户使用存储或文件数超过配额设定，需要用户对数据进行清理到磁盘配额软限制以下方可继续使用。\nQ：作业运行提示“forrtl: Input/output error”\nA：可能是存储某一时刻压力较大，造成IO错误，请您重新提交作业。\nQ：作业运行时报错：forrtl: No space left on device，forrtl: severe (38): error during write, unit 12，但是同样的作业再次提交时可能就正常运行完成。\nA：该问题主要由文件系统中某一OST存储已满导致，请联系与您对接的工程师或系统管理员。\nLustre文件系统由若干IO服务器（Object Storage Services）和Object Storage Targets(OST)组成。当对一个文件进行读写操作时，为了提高IO效率，文件系统会自动将该文件的读写操作分割成多个，在多个OST上并发实现。如果在该过程中，使用到的某一OST出现问题，就会发生读写错误。\nQ:我使用ls命令查看目录下的文件，可是一直停留下那里，没有显示。\nA:遇到这个问题，您可以等待一会，再重新使用ls命令查看目录文件。\n原因之一可能是TH-HPC的登录节点负载比较重，造成使用终端命令受到影响；原因之二可能是用户客户端的网络负载比较重，出现比较严重的网络延迟；原因之三可能是TH-HPC系统的存储正在进行恢复调整。\n6.6 GPU使用问题\nQ：使用CUDA toolkit编译程序后，在gpu_test分区提交作业，运行时提示错误：no CUDA-capable device is detected\nA：可能原因有二种情况：\n原因之一可能是分配到的该计算结点上用于连接CPU与GPU的PCIe总线松动，导致无法找到device。解决方法：在提交作业时",
        ":3200030:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588675] [cn1945:3200030:0]     glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588722] [cn1945:3200030:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.588758] [cn1945:3200030:0]     glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.680342] [cn1945:3200043:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.680526] [cn1945:3200043:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.680558] [cn1945:3200043:0]     glex_md.c:362 UCX ERROR GLEX cre\n[1727595377 680586] [cn1945:3200043:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377 680609] [cn1945:3200043:0]     glex_md.c:362 UCX ERROR GLEX cre\n[1727595377.680647] [cn1945:3200043:0]      glex_md.c:362 UCX ERROR GLEX cre:\n[1727595377.680671] [cn1945:3200043:0]     glex_md.c:362 UCX ERROR GLEX cre:\nyhru\nslurmstepd: error:\ncn1945: task 3: Aborted\nmpi/pmix_v3: _errhandler: cn1945 [1]: pmixp_client_v2.\nerror:\n2210:\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nsate region\nError handler invoked:\nslurmstepd: error: *** STEP 3596459. ON cn1944 CANCELLED AT",
        "时加入了-xHOST），那程序就会根据登陆节点的CPU配置信息进行优化编译，然而程序的运行是在计算节点上，计算节点的CPU配置信息可能不支持AVX，就与登陆节点不同，就会报上面的提示错误。\n解决方法：编译时去掉-xHOST/-xAVX选项，使用其他优化选项。\n备注：-xHost will cause icc/icpc or icl to check the cpu information and find the highest level of extended instructions support to use.\n天河登陆节点ln1、ln2、ln3上的CPU配置信息flag均无avx，ln8、ln9上均有avx。\n如果在ln8或ln9上安装软件时，configure后一定要检查下编译flag是否加入了-xHOST，如果加入，请修改对应的configure文件，将-xHOST删除\n6.5 存储问题\nQ：登陆系统时提示“Some errors happened when getting quota info”\nA：这是由于在对系统进行调整时登陆结点quota服务没有启用导致，对用户本身的操作和作业不会有影响，管理员会定时对此进行调整，请放心使用。\n解决方法：这是因为登陆节点quota服务没有启用，对用户本身的操作和作业不会有影响。\n值班人员每天定时对vol-th、WORK、vol6的quota进行检查，尤其是存储出现问题后（如ost重启），quota会异常关闭，以vol-th为例重启方式如下：\nlfsquotaoff -ug /vol-th\nlfsquotaon -ug /vol-th\nQ：默认的磁盘配额是多少？磁盘配额的含义是什么？\nA：为了合理利用有限的存储资源，目前中心对用户默认进行存储软限制500G，存储硬限制1T，文件数软限制100万，文件数硬限制200万的磁盘配额限制。以磁盘存储为例说明软、硬限制的含义，文件数软、硬限制的含义与其一样。用户使用存储低于500G时，存储状态正常；当用户使用存储介于500G和1T之间时，用户配额异常，通过“lfs quota –g username /vol-th”查看账号配额会看到已使用存储的数字旁边有一个“*”号，状态“4w1d23h59m57s”表示一个月的倒计时，如果用户在倒计时结束前将使用"
    ]
}