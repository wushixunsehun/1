{
    "query": "用户需在HPC4系统上提交一个MPI+OpenMP混合并行作业，要求使用2个计算节点，每个节点运行4进程，每进程开启9线程。请编写对应的作业脚本并说明参数含义。",
    "summaries": [
        "ex系统通过Singularity容器运行HPC系统的MPI程序。需复制Intel编译器、IntelMPI及依赖库至本地，并设置环境变量。提交脚本中需配置PERL、PATH、LD_LIBRARY_PATH等，确保容器内正确加载依赖。关键参数包括`I_MPI_SHM_LMT=shm`、`SLURM_MPI_TYPE=pmi2`，避免MPI错误。使用HPC镜像`/fs2/software/node/redhat-7.2.sif`执行`par.exe`。",
        "本文档记录了在HPC4平台上编译和配置LAMMPS与Kokkos、Plumed的步骤。用户需加载MPI环境，下载并编译Plumed模块，可选择自动或手动编译方式。同时需修改Makefile.linux以适配GPU环境，并编译LAMMPS时启用相关模块。运行前需生成Plumed的so文件。脚本示例展示了如何提交作业使用LAMMPS。注意Kokkos仅支持OpenMPI或MPICH，且某些版本的nvhpv存在兼容性问题。文档还提供了修改后的Install.py内容以解决下载问题。",
        "在HPC4平台上，使用Matlab单节点运行多个作业可通过编写脚本实现。脚本中调用多个matlab命令，分别执行不同的任务，并使用绝对路径确保程序正确运行。每个作业在后台运行，最后通过wait等待所有作业完成。注意路径需使用绝对路径。"
    ],
    "contents": [
        "shutil\nfrom argparse import ArgumentParser\nsys.path.append('..')\nfrom install_helpers import get_cpus, fullpath, geturl, checkmd5sum, getfallback\nparser = ArgumentParser(prog='Install.py',\ndescription=\"LAMMPS library build wrapper script\")\n# settings\nversion = \"2.8.1\"\nmode = \"static\"\n# help message\nHELP = \"\"\"\nSyntax from src dir: make lib-plumed args=\"-b\"\nor: make lib-plumed args=\"-b -v 2.4.3\"\nor: make lib-plumed args=\"-p /usr/local/plumed2 -m shared\"\nSyntax from lib dir: python Install.py -b -v 2.4.3\nor: python Install.py -b\nor: python Install.py -p /usr/local/plumed2 -m shared\nExample:\nmake lib-plumed args=\"-b\"   # download/build in lib/plumed/plumed2\nmake lib-plumed args=\"-p $HOME/plumed2 -m shared\" # use existing Plumed2 installation in $HOME/plumed2\n\"\"\"\n# known checksums for different PLUMED versions. used to validate the download.\nchecksums = { \\\n'2.4.2' : '88188743a6e03ef076e5377d03ebb0e7', \\\n'2.4.3' : 'b1be7c48971627febc11c61b70767fc5', \\\n'2.4.4' : '71ed465bdc7c2059e282dbda8d564e71', \\\n'2.5.0' : '6224cd089493661e19ceacccd35cf911', \\\n'2.5.1' : 'c2a7b519e32197a120cdf47e0f194f81', \\\n'2.5.2' : 'bd2f18346c788eb54e1e52f4f6acf41a', \\\n'2.5.3' : 'de30d6e7c2dcc0973298e24a6da24286', \\\n'2.5.4' : 'f31b7d16a4be2e30aa7d5c19c3d37853', \\\n'2.5.7' : '1ca36226fdb8110b1009aa61d615d4e5', \\\n'2.6.0' : '204d2edae58d9b10ba3ad460cad64191",
        "ex系统使用singularity运行hpc系统mpi程序\n**标签**: singularity\n**创建时间**: 2023-08-29 15:19:56\n**更新时间**: 2023-08-29 16:11:06\n**作者**: 李跃岩\nex系统使用singularity运行hpc系统mpi程序\n这里使用hpc系统使用intel_compiler 18编译的par.exe举例\n复制环境\n将intel编译器的库文件、intelmpi的库文件及可执行文件都拷贝过来，例如拷贝到：\n`${HOME}/intel18ddd`和`${HOME}/dddmpi18`中来，另外由于par.exe需要metis.so，所以把hpc系统的这个库也拷过来，例如拷贝到：`${HOME}/metis-5.1.0-icc18`，下面将要在ex系统通过singularity容器，用intelmpi并行运行par.exe\n设置PERL\n可以自己安装，也可以拷贝`/usr/share/perl5`到ex系统，例如拷贝到`${HOME}/perl-5.16.3/lib/5.16.3`\n提交脚本\n这里以提交到cp6节点为例，提交脚本如下：\n#!/bin/sh\n#SBATCH -n 256\n#SBATCH -p cp6\nmodule add singularity/3.11.0\nexport PERLLIB=${HOME}/perl-5.16.3/lib/5.16.3:${HOME}/perl-5.16.3/lib/5.16.3/CGI\nexport PATH=${HOME}/dddmpi18/bin:${PATH}\nexport LD_LIBRARY_PATH=${HOME}/dddmpi18/lib:${HOME}/intel18ddd/intel64_lin:${HOME}/metis-5.1.0-icc18:${LD_LIBRARY_PATH}\nexport SLURM_MPI_TYPE=pmi2\nsrun singularity exec  env I_MPI_SHM_LMT=shm env PERLLIB=${PERLLIB} env LD_LIBRARY_PATH=${LD_LIBRARY_PATH} env PATH=${PATH} workdir=${PWD}  /fs2/software/node/redhat-7.2.sif ./par.exe\n脚本解释\n1. `env` 可以通过这个参数将",
        "【已解决】HPC4 lammps-kokkos-plumed\n**标签**: lammps，kokkos，plumed\n**创建时间**: 2024-09-20 15:44:26\n**更新时间**: 2024-09-20 16:40:00\n**作者**: 梁言\n环境\nmodule load MPI/openmpi/4.1.3-cuda-gcc11.5.0\n#plumed模块\ncd lib/plumed\nwget https://download.lammps.org/thirdparty/plumed-plumed-src-2.8.2.tgz\n可单独编译plumed，也可以自动编译，自动编译需要修改Install.py ，否则会因为网络问题导致下载出错\n自动编译：\ncd src\nmake lib-plumed args=\"-b -v2.8.2 -m shared\"\n单独编译：prefix需要在公共路径，后面单独编译cpp文件会调用plumed，复制会保留源路径，访问不到\nCC=mpicc CXX=mpicxx FC=mpif90 ./configure prefix=/fs1/software/lammps/2Aug2023-kokkos-plumed-cuda11.8/plumed-install enable-modules=all enable-static-patch enable-mpi\nmake && make install\ncd src\nmake lib-plumed args=\"-p /fs1/home/liangyan/lammps/lammps-2Aug2023-new/kokkos-cuda/openmpi-cuda/lammps-2Aug2023/lib/plumed/plumed-install -m shared\"\n#gpu模块\ncd lib/gpu\nvim 修改Makefile.linux\nCUDA_HOME = /fs1/software/cuda-11.8/\nCUDA_ARCH = -arch=sm_80\nmake -f Makefile.linux -j\ncd src\nmake yes-KSPACE yes-MANYBODY yes-MOLECULE yes-RIGID yes-CLASS2 yes-MC yes-REAXFF yes-REPLICA yes-PLUGIN yes-REACTION yes-PLUMED yes-EXTRA-COMPUTE yes-EXTRA-DUMP yes-EXTRA-FIX yes-KOKKOS yes-gpu yes-KSPACE yes-MANYBODY yes-MOLECULE yes-RIGID yes-REAXFF yes-CLASS2 yes-kokkos\nmake kokkos_",
        "EXTRA-DUMP yes-EXTRA-FIX yes-KOKKOS yes-gpu yes-KSPACE yes-MANYBODY yes-MOLECULE yes-RIGID yes-REAXFF yes-CLASS2 yes-kokkos\nmake kokkos_cuda_mpi -j20\n这个用户计算前需要单独编译.cpp，生成so文件\nplumed mklib ReweightGeomFES.cpp\n#https://www.plumed.org/doc-v2.9/user-doc/html/_l_o_a_d.html\n#脚本示例\n#!/bin/bash\n#SBATCH partition=gpu3\n#SBATCH -N 1\n#SBATCH gpus-per-node=1\n#SBATCH cpus-per-gpu=8\nmodule purge\nmodule load lammps/2Aug2023-kokkos-plumed-cuda11.8\nexport OMP_NUM_THREADS=1\nnvidia-smi dmon > nvi_1.log &\nmpirun -np 1 lmp_kokkos_cuda_mpi -k on g 1 -sf kk -in acc.lmp\n#注\nkokkos 只能用openmpi或者mpich编译 intel不支持。\nnvhpv/22.11  23.11 编译kokkos-plumed 运行会有问题。\n22.11 报错you are trying to use an MPI function, but PLUMED has been compiled without MPI support\n23.11 报错free():double free detected in tcache 2\n参考https://zhuanlan.zhihu.com/p/603892794\n修改后的Install.py如下\n#!/usr/bin/env python\n\"\"\"\nInstall.py tool to download, unpack, build, and link to the plumed2 library\nused to automate the steps described in the README file in this dir\n\"\"\"\nfrom future import print_function\nimport sys, os, platform, subprocess, shutil\nfrom argparse import ArgumentParser\nsys.path.append('..')\nfrom install_helpers import get_cpus, fullpath, geturl, checkmd5sum, getfallback\nparser = ArgumentParser",
        "where args are comannd line arguments for mpiexec (see below),\nexecutable is the name of the eecutable and pgmargs are command line\narguments for the executable. For example the following command will run\nthe MPI progam a.out on 4 processes:\nmpiexec.slurm -n 4 a.out\nmpiexec.slurm supports the following options:\n[-n nprocs]\n[-host hostname]\n[-verbose]\n[-nostdin]\n[-allstdin]\n[-nostdout]\n[-pernode]\n[-config config_file]\n[-help|-?]\n[-man]\n5. `/fs2/software/node/redhat-7.2.sif` 这个是hpc系统的镜像\n6. `SLURM_MPI_TYPE=pmi2` 设置这个或设置`mpi=pmi2`，否则将使用glex网\n7. 若使用glex网，因为pmi版本不一致，会报错【TODO】\n[cn76966:1758336] PMIX ERROR: NOT-FOUND in file client/pmix_client.c at line 562\nAbort(672779791): Fatal error in internal_Init: Other MPI error, error stack:\ninternal_Init(59)....: MPI_Init(argc=(nil), argv=(nil)) failed\nMPII_Init_thread(209):\nMPID_Init(359).......:\nMPIR_pmi_init(152)...: PMIX_Init returned -46",
        "【已解决】HPC4 matlab单节点运行多个作业\n**标签**: 无标签\n**创建时间**: 2024-12-10 11:28:56\n**更新时间**: 2024-12-10 11:28:56\n**作者**: 杜思慧\n**1.脚本**\n#!/bin/bash\nmodule add loginnode\nmatlab -nodesktop -nosplash -logfile 1.log -r \"addpath('/fs1/home/daimx/work/matlab/m1'); testm1; exit\" &\nmatlab -nodesktop -nosplash -logfile 2.log -r \"addpath('/fs1/home/daimx/work/matlab/m2'); testm2; exit\" &\nmatlab -nodesktop -nosplash -logfile 3.log -r \"addpath('/fs1/home/daimx/work/matlab/m3'); testm3; exit\" &\nwait\n**2.注意事项**\n程序中的路径需要全部改为绝对路径",
        "PATH=${PATH} workdir=${PWD}  /fs2/software/node/redhat-7.2.sif ./par.exe\n脚本解释\n1. `env` 可以通过这个参数将环境送入singularity容器中\n2. `I_MPI_SHM_LMT=shm` 若不加将报错\nFatal error in PMPI_Waitall: Other MPI error, error stack:\nPMPI_Waitall(405)...............: MPI_Waitall(count=7, req_array=0x3d088a0, status_array=0x3d08940) failed\nMPIR_Waitall_impl(221)..........: fail failed\nPMPIDI_CH3I_Progress(623).......: fail failed\npkt_RTS_handler(317)............: fail failed\ndo_cts(662).....................: fail failed\nMPID_nem_lmt_dcp_start_recv(302): fail failed\ndcp_recv(165)...................: Internal MPI error!  Cannot read from remote process\nTwo workarounds have been identified for this issue:\n1) Enable ptrace for non-root users with:\necho 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n2) Or, use:\nI_MPI_SHM_LMT=shm\n3. `PERLLIB` 若不加将报错\nCan't locate Switch.pm in @INC (@INC contains: /usr/lib64/perl5 /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/share/perl5 .) at /usr/bin/mpiexec line 49.\n4. `PATH` 若不加将报错\nUnknown option: pmi_args\nUsage:\nmpiexec.slurm args executable pgmargs\nwhere args are comannd line arguments for mpiexec (see below),\nexecutable is the name of the eecutable and pgmargs are command line\narguments"
    ]
}